{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alexei/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package inaugural to /home/alexei/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/inaugural.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "nltk.download('punkt')\n",
    "nltk.download('inaugural')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading yelp: Package 'yelp' not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('yelp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Working with text!</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Sentiment Analysis</h2>\n",
    "Identify entities and emotions in a sentence and use these to determine if the entity is being viewed positively or negatively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Easy examples</h3>\n",
    "<li>I had an <b style=\"color:green\">excellent</b> souffle at the restaurant Cavity Maker</li>\n",
    "<li>Excellent is a positive word for both the souffle as well as for the restaurant</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Not so easy examples</h3>\n",
    "<h4>Often, looking at words alone is not enough to figure out the sentiment</h4>\n",
    "<li><i>The Girl on the Train is an <span style=\"color:green\">excellent</span> book for a ‘stuck at home’ snow day</i></li> This one is easy since it includes an explicit positive opinion using a positive word\n",
    "<li><i>The Girl on the Train is an <span style=\"color:green\">excellent</span> book for using as a liner for your cat’s litter box</i></li> Not so simple! The positive word \"excellent\" is used with a negative connotation. \n",
    "<li><i>The Girl on the Train is <span style=\"color:green\">better</span> than Gone Girl</i></li> The positive word is used as a comparator. Whether the writer likes The Girl on the Train or not depends on what he or she thinks of Gone Girl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Bottom line</h4>\n",
    "Sentiment analysis is generally a starting point in analyzing a text and is then coupled with other techniques (e.g., topic analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Sentiment analysis is usually done using a corpus of positive and negative words</h2>\n",
    "<li>Some sources compile lists of positive and negative words\n",
    "<li>Others include the polarity - the degree of positivity or negativity - of each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Sources of sentiment coded words</h2>\n",
    "<ol>\n",
    "<li>Hu and Liu's sentiment analysis lexicon: words coded as either positive or negative</li>\n",
    "<ul>\n",
    "<li>http://ptrckprry.com/course/ssd/data/positive-words.txt\n",
    "<li>http://ptrckprry.com/course/ssd/data/negative-words.txt\n",
    "</ul>\n",
    "<li>NRC Emotion Lexicon: words coded into emotional categories (many languages)</li>\n",
    "<ul>\n",
    "<li>http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm</li>\n",
    "</ul>\n",
    "<li>SentiWordNet: Lists of words weighted by positive or negative sentiment. Includes guidance on how to use the words</li>\n",
    "<ul>\n",
    "<li>http://sentiwordnet.isti.cnr.it/</li>\n",
    "</ul>\n",
    "<li>Vadar Sentiment tool: 7800 words with positive or negative polarity</li>\n",
    "<ul>\n",
    "<li>Included with python nltk</li>\n",
    "</ul>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Our examples</h2>\n",
    "<li>Compiled set of 15 reviews each of four neighborhood restaurants\n",
    "<li>Presidential inaugural addresses (from Washington to Trump)\n",
    "<li>Some data from yelp (very limited!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Simple sentiment analysis</h3>\n",
    "Compute the proportion of positive and negative words in a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(url):\n",
    "    import requests\n",
    "    words = requests.get(url).content.decode('latin-1')\n",
    "    word_list = words.split('\\n')\n",
    "    index = 0\n",
    "    while index < len(word_list):\n",
    "        word = word_list[index]\n",
    "        if ';' in word or not word:\n",
    "            word_list.pop(index)\n",
    "        else:\n",
    "            index+=1\n",
    "    return word_list\n",
    "\n",
    "#Get lists of positive and negative words\n",
    "p_url = 'http://ptrckprry.com/course/ssd/data/positive-words.txt'\n",
    "n_url = 'http://ptrckprry.com/course/ssd/data/negative-words.txt'\n",
    "positive_words = get_words(p_url)\n",
    "negative_words = get_words(n_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Read the text being analyzed and count the proportion of positive and negative words in the text</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/community.txt','r') as f:\n",
    "    community = f.read()\n",
    "with open('data/le_monde.txt','r') as f:\n",
    "    le_monde = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Compute sentiment by looking at the proportion of positive and negative words in the text</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "community 5.09%\t 1.12%\t 3.97%\n",
      "le monde  5.33%\t 1.49%\t 3.85%\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "cpos = cneg = lpos = lneg = 0\n",
    "for word in word_tokenize(community):\n",
    "    if word in positive_words:\n",
    "        cpos+=1\n",
    "    if word in negative_words:\n",
    "        cneg+=1\n",
    "for word in word_tokenize(le_monde):\n",
    "    if word in positive_words:\n",
    "        lpos+=1\n",
    "    if word in negative_words:\n",
    "        lneg+=1\n",
    "print(\"community {0:1.2f}%\\t {1:1.2f}%\\t {2:1.2f}%\".format(cpos/len(word_tokenize(community))*100,\n",
    "                                                        cneg/len(word_tokenize(community))*100,\n",
    "                                                        (cpos-cneg)/len(word_tokenize(community))*100))\n",
    "print(\"le monde  {0:1.2f}%\\t {1:1.2f}%\\t {2:1.2f}%\".format(lpos/len(word_tokenize(le_monde))*100,\n",
    "                                                        lneg/len(word_tokenize(le_monde))*100,\n",
    "                                                        (lpos-lneg)/len(word_tokenize(le_monde))*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Simple sentiment analysis using NRC data</h2>\n",
    "<li>NRC data codifies words with emotions</li>\n",
    "<li>14,182 words are coded into 2 sentiments and 8 emotions</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>For example, the word abandonment is associated with anger, fear, sadness and has a negative sentiment</h4>\n",
    "<li>abandoned\tanger\t1\n",
    "<li>abandoned\tanticipation\t0\n",
    "<li>abandoned\tdisgust\t0\n",
    "<li>abandoned\tfear\t1\n",
    "<li>abandoned\tjoy\t0\n",
    "<li>abandoned\tnegative\t1\n",
    "<li>abandoned\tpositive\t0\n",
    "<li>abandoned\tsadness\t1\n",
    "<li>abandoned\tsurprise\t0\n",
    "<li>abandoned\ttrust\t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Read the NRC sentiment data</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrc = \"data/NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt\"\n",
    "count=0\n",
    "emotion_dict=dict()\n",
    "with open(nrc,'r') as f:\n",
    "    all_lines = list()\n",
    "    for line in f:\n",
    "        if count < 46:\n",
    "            count+=1\n",
    "            continue\n",
    "        line = line.strip().split('\\t')\n",
    "        if int(line[2]) == 1:\n",
    "            if emotion_dict.get(line[0]):\n",
    "                emotion_dict[line[0]].append(line[1])\n",
    "            else:\n",
    "                emotion_dict[line[0]] = [line[1]]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Functionalize this</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nrc_data():\n",
    "    nrc = \"data/NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt\"\n",
    "    count=0\n",
    "    emotion_dict=dict()\n",
    "    with open(nrc,'r') as f:\n",
    "        all_lines = list()\n",
    "        for line in f:\n",
    "            if count < 46:\n",
    "                count+=1\n",
    "                continue\n",
    "            line = line.strip().split('\\t')\n",
    "            if int(line[2]) == 1:\n",
    "                if emotion_dict.get(line[0]):\n",
    "                    emotion_dict[line[0]].append(line[1])\n",
    "                else:\n",
    "                    emotion_dict[line[0]] = [line[1]]\n",
    "    return emotion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger', 'fear', 'negative', 'sadness']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_dict = get_nrc_data()\n",
    "emotion_dict['abandoned']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Analyzing yelp reviews</h2>\n",
    "<h4>Caveat: We're only looking at one review snippet for each restaurant</h4>\n",
    "<ol>\n",
    "<li>Download yelp python \"pip install yelp\"\n",
    "<li>Register with yelp https://www.yelp.com/developers/manage_api_keys (use anything for the host)\n",
    "<li>Copy the various keys into variables as below\n",
    "</ol>\n",
    "<h4>We'll see what we can figure out re reviews of restaurants close to Columbia</h4>\n",
    "<h4>First let's read in the yelp keys</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSUMER_KEY = \"\"\n",
    "CONSUMER_SECRET = \"\"\n",
    "TOKEN = \"\"\n",
    "TOKEN_SECRET = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>I've saved my keys in a file and will use those. Don't run the next cell!</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'yelp_keys.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ac9324146dd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yelp_keys.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mCONSUMER_KEY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'yelp_keys.txt'"
     ]
    }
   ],
   "source": [
    "with open('yelp_keys.txt','r') as f:\n",
    "    count = 0\n",
    "    for line in f:\n",
    "        if count == 0:\n",
    "            CONSUMER_KEY = line.strip()\n",
    "        if count == 1:\n",
    "            CONSUMER_SECRET = line.strip()\n",
    "        if count == 2:\n",
    "            TOKEN = line.strip()\n",
    "        if count == 3:\n",
    "            TOKEN_SECRET = line.strip()\n",
    "        count+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>We need to do a few things</h4>\n",
    "<ul>\n",
    "<li>Get the latitude and longitude of our location\n",
    "<li>Set up the parameters for what data we want from yelp\n",
    "<li>Query yelp by passing authentication info as well as our parameters\n",
    "<li>Extract review snippets\n",
    "<li>And append into a containing (restaurant,review snippet) tuples\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YELP - FIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"Emeritus\")\n",
    "location = geolocator.geocode(\"Columbia University\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40.8079488, -73.96179735775709)\n"
     ]
    }
   ],
   "source": [
    "print((location.latitude, location.longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll use the get_lat_lng function we wrote way back in week 3\n",
    "# def get_lat_lng(address):\n",
    "#     url = 'https://maps.googleapis.com/maps/api/geocode/json?address='\n",
    "#     url += address\n",
    "#     import requests\n",
    "#     response = requests.get(url)\n",
    "#     if not (response.status_code == 200):\n",
    "#         return None\n",
    "#     data = response.json()\n",
    "#     if not( data['status'] == 'OK'):\n",
    "#         return None\n",
    "#     main_result = data['results'][0]\n",
    "#     geometry = main_result['geometry']\n",
    "#     latitude = geometry['location']['lat']\n",
    "#     longitude = geometry['location']['lng']\n",
    "#     return latitude,longitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = geolocator.geocode(\"Columbia University\")\n",
    "lat,long = (location.latitude, location.longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40.8079488, -73.96179735775709)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat, long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now set up our search parameters\n",
    "def set_search_parameters(lat,long,radius):\n",
    "  #See the Yelp API for more details\n",
    "    params = {}\n",
    "    params[\"term\"] = \"restaurant\"\n",
    "    params[\"ll\"] = \"{},{}\".format(str(lat),str(long))\n",
    "    params[\"radius_filter\"] = str(radius) #The distance around our point in metres\n",
    "    params[\"limit\"] = \"10\" #Limit ourselves to 10 results\n",
    " \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = set_search_parameters(lat,long,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'term': 'restaurant',\n",
       " 'll': '40.8079488,-73.96179735775709',\n",
       " 'radius_filter': '200',\n",
       " 'limit': '10'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Client_ID = 'GwCLWYFRDTA9H3uz4S6-Lw'\n",
    "API_Key = 'TkpZ7BzkQmRG32BoDJ2GN0GhrrbXe93asSjtBhD8t6tRp_o6TvW27t4rhdQHwhjfHJfG-hioapLukJQgiCid3vdlXUgSQrkrfO-hxaDBzsQYOpe_IIsvW7ab1eFDX3Yx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Write the function that queries yelp. We'll use rauth library to handle authentication</h4>\n",
    "!pip install rauth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yelp.client import Client\n",
    "MY_API_KEY = \"TkpZ7BzkQmRG32BoDJ2GN0GhrrbXe93asSjtBhD8t6tRp_o6TvW27t4rhdQHwhjfHJfG-hioapLukJQgiCid3vdlXUgSQrkrfO-hxaDBzsQYOpe_IIsvW7ab1eFDX3Yx\" #  Replace this with your real API key\n",
    "client = Client(MY_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://api.yelp.com/v3/businesses/search'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The status code is 400\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "import json\n",
    "req=requests.get(url, params=params)\n",
    " \n",
    "# proceed only if the status code is 200\n",
    "print('The status code is {}'.format(req.status_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': {'code': 'VALIDATION_ERROR',\n",
       "  'description': 'Authorization is a required parameter.',\n",
       "  'field': 'Authorization',\n",
       "  'instance': None}}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# printing the text from the response \n",
    "json.loads(req.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(params):\n",
    "    import rauth\n",
    "    consumer_key = CONSUMER_KEY\n",
    "    consumer_secret = CONSUMER_SECRET\n",
    "    token = TOKEN\n",
    "    token_secret = TOKEN_SECRET\n",
    "\n",
    "    session = rauth.OAuth1Session(\n",
    "    consumer_key = consumer_key\n",
    "    ,consumer_secret = consumer_secret\n",
    "    ,access_token = token\n",
    "    ,access_token_secret = token_secret)\n",
    "\n",
    "    request = session.get(\"http://api.yelp.com/v2/search\",params=params)\n",
    "    #Transforms the JSON API response into a Python dictionary\n",
    "    data = request.json()\n",
    "    session.close()\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rauth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-263c80795559>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"40.8059262,-73.967972\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mrauth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_search_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"40.8059262,-73.967972\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"40.8059262,-73.967972\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rauth'"
     ]
    }
   ],
   "source": [
    "(\"40.8059262,-73.967972\")\n",
    "import rauth\n",
    "response = get_results(set_search_parameters((\"40.8059262,-73.967972\")[0],(\"40.8059262,-73.967972\")[1],200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'url' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-18d3c3b28e89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Get the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_search_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_lat_lng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Community Food and Juice\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mget_lat_lng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Community Food and Juice\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-11019a8b34c3>\u001b[0m in \u001b[0;36mget_lat_lng\u001b[0;34m(address)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_lat_lng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#url = 'https://maps.googleapis.com/maps/api/geocode/json?address='\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0murl\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maddress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'url' referenced before assignment"
     ]
    }
   ],
   "source": [
    "#Get the results\n",
    "response = get_results(set_search_parameters(get_lat_lng(\"Community Food and Juice\")[0],get_lat_lng(\"Community Food and Juice\")[1],200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Extract snippets</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-f8ef43cb6a7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mall_snippets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbusiness\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'businesses'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbusiness\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msnippet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbusiness\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'snippet_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbusiness\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "all_snippets = list()\n",
    "for business in response['businesses']:\n",
    "    name = business['name']\n",
    "    snippet = business['snippet_text']\n",
    "    id = business['id']\n",
    "    all_snippets.append((id,name,snippet))\n",
    "all_snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Functionalize this</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snippets(response):\n",
    "    all_snippets = list()\n",
    "    for business in response['businesses']:\n",
    "        name = business['name']\n",
    "        snippet = business['snippet_text']\n",
    "        id = business['id']\n",
    "        all_snippets.append((id,name,snippet))\n",
    "    return all_snippets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>A function that analyzes emotions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_analyzer(text,emotion_dict=emotion_dict):\n",
    "    #Set up the result dictionary\n",
    "    emotions = {x for y in emotion_dict.values() for x in y}\n",
    "    emotion_count = dict()\n",
    "    for emotion in emotions:\n",
    "        emotion_count[emotion] = 0\n",
    "\n",
    "    #Analyze the text and normalize by total number of words\n",
    "    total_words = len(text.split())\n",
    "    for word in text.split():\n",
    "        if emotion_dict.get(word):\n",
    "            for emotion in emotion_dict.get(word):\n",
    "                emotion_count[emotion] += 1/len(text.split())\n",
    "    return emotion_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Now we can analyze the emotional content of the review snippets</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restaurant   fear\ttrust negative positive joy   disgust anticip sadness surprise\n"
     ]
    }
   ],
   "source": [
    "print(\"%-12s %1s\\t%1s %1s %1s %1s   %1s %1s %1s %1s\"%(\n",
    "        \"restaurant\",\"fear\",\"trust\",\"negative\",\"positive\",\"joy\",\"disgust\",\"anticip\",\n",
    "        \"sadness\",\"surprise\"))\n",
    "        \n",
    "for snippet in all_snippets:\n",
    "    text = snippet[2]\n",
    "    result = emotion_analyzer(text)\n",
    "    print(\"%-12s %1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\"%(\n",
    "        snippet[1][0:10],result['fear'],result['trust'],\n",
    "          result['negative'],result['positive'],result['joy'],result['disgust'],\n",
    "          result['anticipation'],result['sadness'],result['surprise']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Let's functionalize this</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restaurant           fear\ttrust negative positive joy   disgust anticip sadness surprise\n"
     ]
    }
   ],
   "source": [
    "def comparative_emotion_analyzer(text_tuples):\n",
    "    print(\"%-20s %1s\\t%1s %1s %1s %1s   %1s %1s %1s %1s\"%(\n",
    "            \"restaurant\",\"fear\",\"trust\",\"negative\",\"positive\",\"joy\",\"disgust\",\"anticip\",\n",
    "            \"sadness\",\"surprise\"))\n",
    "        \n",
    "    for text_tuple in text_tuples:\n",
    "        text = text_tuple[2] \n",
    "        result = emotion_analyzer(text)\n",
    "        print(\"%-20s %1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\"%(\n",
    "            text_tuple[1][0:20],result['fear'],result['trust'],\n",
    "              result['negative'],result['positive'],result['joy'],result['disgust'],\n",
    "              result['anticipation'],result['sadness'],result['surprise']))\n",
    "        \n",
    "#And test it        \n",
    "comparative_emotion_analyzer(all_snippets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>And let's functionalize the yelp stuff as well</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-6123d7f67944>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#And test it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0manalyze_nearby_restaurants\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Community Food and Juice\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-6123d7f67944>\u001b[0m in \u001b[0;36manalyze_nearby_restaurants\u001b[0;34m(address, radius)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0manalyze_nearby_restaurants\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mradius\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mlat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlong\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_lat_lng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_search_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mradius\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msnippets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_snippets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "def analyze_nearby_restaurants(address,radius):\n",
    "    lat,long = get_lat_lng(address)\n",
    "    params = set_search_parameters(lat,long,radius)\n",
    "    response = get_results(params)\n",
    "    snippets = get_snippets(response)\n",
    "    comparative_emotion_analyzer(snippets)\n",
    "\n",
    "#And test it    \n",
    "analyze_nearby_restaurants(\"Community Food and Juice\",200)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test it on some other place\n",
    "analyze_nearby_restaurants(\"221 Baker Street\",200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Simple analysis: Word Clouds</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Let's see what sort of words the snippets use</h4>\n",
    "<li>First we'll combine all snippets into one string\n",
    "<li>Then we'll generate a word cloud using the words in the string\n",
    "<li>You may need to install wordcloud using pip\n",
    "<li>pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=''\n",
    "for snippet in all_snippets:\n",
    "    text+=snippet[2]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "We need at least 1 word to plot a word cloud, got 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-90d24dd9a3bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mwordcloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTOPWORDS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbackground_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'white'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ds/eme/pro/my_project_env/lib/python3.6/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m         \"\"\"\n\u001b[0;32m--> 631\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ds/eme/pro/my_project_env/lib/python3.6/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    611\u001b[0m         \"\"\"\n\u001b[1;32m    612\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ds/eme/pro/my_project_env/lib/python3.6/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             raise ValueError(\"We need at least 1 word to plot a word cloud, \"\n\u001b[0;32m--> 404\u001b[0;31m                              \"got %d.\" % len(frequencies))\n\u001b[0m\u001b[1;32m    405\u001b[0m         \u001b[0mfrequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: We need at least 1 word to plot a word cloud, got 0."
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "wordcloud = WordCloud(stopwords=STOPWORDS,background_color='white',width=3000,height=3000).generate(text)\n",
    "\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Let's do a detailed comparison of local restaurants</h2>\n",
    "<h4>I've saved a few reviews for each restaurant in four directories</h4>\n",
    "<h4>We'll use the PlainTextCorpusReader to read these directories</h4>\n",
    "<li>PlainTextCorpusReader reads all matching files in a directory and saves them by file-ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "community_root = \"data/community\"\n",
    "le_monde_root = \"data/le_monde\"\n",
    "community_files = \"community.*\"\n",
    "le_monde_files = \"le_monde.*\"\n",
    "heights_root = \"data/heights\"\n",
    "heights_files = \"heights.*\"\n",
    "amigos_root = \"data/amigos\"\n",
    "amigos_files = \"amigos.*\"\n",
    "community_data = PlaintextCorpusReader(community_root,community_files)\n",
    "le_monde_data = PlaintextCorpusReader(le_monde_root,le_monde_files)\n",
    "heights_data = PlaintextCorpusReader(heights_root,heights_files)\n",
    "amigos_data = PlaintextCorpusReader(amigos_root,amigos_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amigos.1',\n",
       " 'amigos.10',\n",
       " 'amigos.11',\n",
       " 'amigos.12',\n",
       " 'amigos.13',\n",
       " 'amigos.14',\n",
       " 'amigos.15',\n",
       " 'amigos.16',\n",
       " 'amigos.17',\n",
       " 'amigos.18',\n",
       " 'amigos.19',\n",
       " 'amigos.2',\n",
       " 'amigos.20',\n",
       " 'amigos.21',\n",
       " 'amigos.3',\n",
       " 'amigos.4',\n",
       " 'amigos.5',\n",
       " 'amigos.6',\n",
       " 'amigos.7',\n",
       " 'amigos.8',\n",
       " 'amigos.9']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amigos_data.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I see all these bad reviews, but speaking for myself, I\\'ve never gotten bad service here, or had a negative experience in the least--and I\\'m here about once a week.\\n\\nThe atmosphere is fun and laid back, but not in a divey way. I\\'ve never had a bad dish, although some are merely average; however, a few are A+: THE QUESADILLAS, the Queso Fundido, and the margaritas are among the best I\\'ve ever had. The guacamole is above average.\\n\\nAlso, they have really great specials during the week--I believe one day is something like $21.95 for unlimited quesadillas + 5 margaritas? That sounds so impossible even as I type it but I\\'m pretty sure that\\'s correct\\nMy friend and I decided to go out on Cinco de Mayo but we wanted something close to home with a nice vibe, people and good music and food. I remember a friend telling me about this place a while back. So I decided to make a reservation.  Not only did we get there and find that it was packed BUT the receptionist explained that since we did make reservations,  we were priority on the list. \\n\\nWe found some seats at the bar and got some drinks. No more than 20 minutes later were we seated. The staff was great,  the food was great,  the drinks were AMAZING! The music was beautiful and the vibe was on point. \\n\\nI will DEFINITELY be going back with even more people! I love it!\\n\\nWe stopped at Amigos while walking the length of Broadway (yes, all 13 miles of it) and were craving a quick bite for lunch. It\\'s located on a cute and busy strip along Broadway, not far from Bank Street and a few doors down from the Seinfeld famous Tom\\'s Restaurant.  \\n\\nThey had both sidewalk and indoor seating available when we visited. We opted for indoor near the window. Service was prompt and courteous while we were here--no complaints at all. They provide complimentary chips and salsa which is always a pro.\\n\\nThe menu is ample--for both food and drink options. Definitely a TON of specials (each day has a different food special, brunch specials on the weekends, $3 margaritas during the day). Definitely a place you can get a great bang for your buck.\\n\\nWe both opted for quesadillas (shrimp and chicken) and were surprised to see it came with three medium sized quesadillas, along with sides of rice and beans. Huge portion! Quality was average I\\'d say, we were satisfied but not blown away. Would definitely come here again for happy hour specials though--looked like a fun spot!\\nYeah what a terrible place. The waiter hated us from the second we walked in. We had to ask no less than 5 times over the course of our meal for water refills, at least 3 for more chips, and I had to go get the check myself. Possibly the most blasé service I have had ever, but at least in the last long time. \\n\\nThe food was decent, though, which is surprising considering the horrific service. My shrimp enchiladas had a good flavor if they were drenched in oil. Guac was good and you can customize the spiciness and ingredients. The spicy margarita was really top notch, and at $11 I\\'ve gotten to the point where I think that\\'s a fair price for a cocktail.\\n\\nWhen I went to retrieve the check since we had been sitting for a good 20 minutes, I told the manager what a terrible experience we had. He was like, \"Oh, I\\'m so sorry, service is top notch, I\\'m so sorry, please come back and see us again\" asking for my name and stuff. Yeah I mean I won\\'t be back because I don\\'t live in NY but I would never go back anyway.\\nThe worst service that I have had in a long time. We waited for 20 minutes, had to seat ourselves and get our own menus, and ended up leaving without even getting spoken to or given water. Won\\'t be coming back.\\nAfter walking around the area, we decided to give this place a try. We had a choice of either sitting at the booths or sitting near the bar. We opted to sit near the bar and was immediately seated. The back of the restaurant and the restaurant looked a little sketchy. Daughter ordered the Arroz Con Pollo and thought it was a bit bland. It tasted like Chicken Fried Rice. Hubs ordered the Tilapia Tacos and thought it was pretty good. I liked the fact that the tilapia was fried and the fish wasn\\'t overcooked. I ordered the Enchilada with Salsa Verde and it was spicy and pretty good. Daughter ended up eating my order because it was tastier. She had a little reaction on her lip and scared herself because she thought there was peanuts in my Enchilada entree. We asked our waiter and he went to the kitchen to make sure that there wasn\\'t any in our orders. He reassured us that everything was safe. Turned out that she didn\\'t get a reaction to peanuts. \\n\\nMy enchilada was very spicy and her lips ended up feeling tingly from it and it scared her. But, the waiter was very nice in reassuring her that the food was safe. Service was very friendly, though. Will return to try other items when in the area.\\nwo stars for Amigos : ( \\n\\nI\\'m here in NYC visiting friends for a couple of days . My best friends works in Columbia university area so we decided to have lunch there ! The famous Tom\\'s restaurant was packed so we chose Amigos ! They had happy hour & of course lunch specials .\\n\\nSitting at the bar ... We were overlooked several times by the bartender . He finally came over & dropped off menus & Walked off ... *welcome to New York * lol . Moments like that I wish southern charm & manners ! He eventually came back & asked if we wanted drinks . Water first then we ordered margaritas. They were on special for $5. The size was very small . The quality of the tequila was very much a cheaper option . Ehhh could have passed on that . \\n\\nOn to food ... I ordered the quesadilla special , it came with rice & beans . We sat there for about 15 mins after ordering & no one brought us chips & salsa . We asked & his reply was \" it\\'s only for customers eating food but since I ordered food we could get some . \" I read over the menu to see if that was in small print somewhere .My friends decided they would just nibble on chips until they figured out what they wanted . Owen , who was rude but hey , guess it\\'s the NY thing, assumed my friend was fasting because she didn\\'t order anything . *side eye * we asked for his name & we talked for a bit . He was very cranky because he normally doesn\\'t work that shift & implied he makes more during breakfast lol . Sigh .... So that was the reason behind crappy service ???\\n\\nMy best friend raved about the service & this place prior to walking in . Her usual person wasn\\'t there & she apologized for my experience . I didn\\'t finish my food because the rice & beans seemed microwaved . You can\\'t really mess up a quesadilla ... It was ehhhh ok . Overall, I wouldn\\'t go back to Amigos unless it was the last option in the area . The drinks were subpar ... I left with a little buzz & the urge to eat something else . There\\'s tons of other food options in the Colombia university area to try !\\nAmigos is a nice little Mexican restaurant on the UWS, and conveniently located a block away from work. The first time I went there I had a quesadilla which was just blah. How can someone mess up a quesadilla you might ask? Same question I had. Needless to say, I hadn\\'t gone back to Amigos in over a year. \\n\\nI recently went back with a group of coworkers for drinks and had a more pleasant experience. We went around 4pm on a Friday and the restaurant was pretty empty. I had a peach and a strawberry margarita on the rocks, which were about $5-6. While at first they didn\\'t taste very strong, it definitely caught back up to me later. The strawberry tasted like any stars every margarita that you\\'ve ever had while the peach was actually pretty delicious. We also ordered the guacamole with pineapple and pomegranate and WOW. It was so delicious and the pineapple and pomegranate were the perfect touch. \\n\\nIf you want cheap margaritas and amazing guac, check out Amigos.\\nLate night happy hour is amazing with Erik! He\\'s the best bartender--so attentive and full of energy! \\n\\nCome to Amigos for a great time with great people!\\nMy cousin recommended i try this place out...she knows i love margaritas and burritos. I wasted no time paying Amigos a visit.  It\\'s a really cute place, very spacious. i was somewhat surprised at how big it was. After being seated i was given the complimentary chip and salsa. The chips were warm what i feel is always an excellent touch. I started off with a coconut margarita and it came in a little mason jar. I was a little taken back because it was so small, maybe the size of the glass threw me off. None the less it was very taste, and i had 2 additional. I was craving nachos, but i ordered the fish tacos. The order came with 3 tacos and a side of rice and beans.The portion was a great size i couldn\\'t finish it all. The tacos weren\\'t anything to go cray about, but they were good for the most part. I\\'ll def be back. It\\'s a kool place to have dinner with friends are just grab a few drinks. The waiter was very attentive...i didn\\'t have to flag him down for anything because he constantly checked on my table\\nand those surrounding\\nCame here on a Thursday night with some friends for drinks and food. The place is okay, the decor is cute but the place is getting a bit withered down. The staff is really friendly. The only thing I didn\\'t like is that every time we asked our waiter for something he kind of acted annoyed at times. I had a chicken quesadilla it was really good. I also had two frozen margaritas which were also very delicious but small in size.\\nThis is so far from the REAL Mexican food I\\'m used to in California. Fortunately, the service is very nice. I do enjoy the chips, just wish their salsa was spicier and more flavorful.\\n\\nMy girl loves the food and she\\'s from California as well, I guess she must have been away for too long. Her favorite is the carnitas tacos.\\n\\nWe just had brunch there today and it was so-so. I had the chilaquiles... My friend loved his torta.\\nI thought this place was great, friendly service & reasonable prices. lunch special $10!  I had a quesadilla.  It was good , I don\\'t understand these reviewers who say differently. Enough of you bratty California clowns  - no good mexican in NYC - GO back there.\\nAmigos is so much fun! I went for the quesadillas and margarita Tuesday night special. The quesadillas were okay but the margaritas are definitely worth it! Very strong! We had Matt as our waiter and he was awesome! The manager is so sweet!\\nNah, terrible service.\\n\\nAsked for a drink and she said and I quote \"cry about it, cry about it. Shut up.\"\\n\\nAll I asked for was another drink, I was not saying anything else.  Not cool at all. Solid deal, but mean people. \\nOne Of The Worst Restaurant I\\'ve Ever Been To In My Life! The Waiter Was So Rude. After My Friend And I Sit Down. He Came Over Unfriendly! Asking Are You Ready? Seemed As If He Had Something Better To Do? The Way He Was Acting. We Just Gotten Up In Left Before Ordering. Unbelievable! I Thought Your Going To Be Nice To Make Tips? No???? Never Again\\nThe food was ok the Guac was really tasty the drinks are ok the strawberry margarita was good and the bulldog was so so. The chicken chicharrón burrito I enjoyed a bit. Waiters were attentive and very nice.\\nOne of the worst Mexican restaurants! I love Mexican food and this is very inferior. We live across the street so we keep coming hoping it improves only to be repeatedly disappointed.\\nThey were good for a while when they first opened. It\\'s popular with the students I suppose, which is why they still have a lease with Columbia. Today the chips were burnt. The Queso fundido with chorizo was mostly chorizo. The guacamole didn\\'t taste like anything. The salsa is chopped tomatoes and not much else.\\nPlantains-only a few were ripe... it\\'s hopeless!\\nDo NOT go here if you\\'re looking for good Mexican food, or good food period. Went here with a friend and ate one of the worst meals I\\'ve had in a really long time. The fish bowl we ordered tasted like diluted tequila with a splash of juice -- drank about 1/5 of it. The fajitas I ordered were bland, and what flavor the chicken did have tasted like it came straight from a can. Rice was overlooked and dry, beans were mushy and tasteless. Ate about 1/4 of my food, and my friend, who ordered fish tacos, ate about the same because her dish was so bad. Service was also mediocre...\\n\\nTo top that off, i got sick from what little I ate! Had a stomach bug for about 36 hours :(\\nI was around the area and heard about the good drink and food specials so I decided to check this place out. There was no wait, which is always a plus.\\n\\nMy friend and I both had a frozen Mango Coronarita, which was inexpensive and strong as well. The service was a little slow, but I can make the exception because our waitress told us she just got employed the day before.\\n\\nWe had the guacamole chips and dip for an appetizer, which was pretty good. The best part about this place is that it has soooo many great food and drink specials-DAILY! Recommended for those who love unlimited drinks and sliders!\\nMy friend and I were starving and looking for a place with decently priced margaritas to celebrate the day before Cinco de Mayo. After stopping at a few places, we decided to take advantage of Amigos outdoor seating while we still could (we got to the restaurant at around 5pm). The outdoor tables get good sunlight in the afternoon and evening so we were very comfortable in the 60-something degree weather.\\nOur first happy surprise was our free 6 ounce margaritas we got for checking in on Foursquare. I am usually not a huge fan of frozen margs, but these were excellent and very strong, and the (optional) mango flavoring was a nice addition. Yeah, they\\'re small but they are only $5 (all day, except 11-1pm when they are $3!). I got the larger version later for $10 which is more than double the size of the small version. Before we even had a chance to order drinks, we were brought a basket of chips and chunky slsa, which was excellent and very flavorful.\\nMy friend and I were starving but were planning on drinking and dining for a few hours, so after we started off with the nachos topped with chorizo ($15). The nachos were HUGE and were almost too much for the both of us, but we manned (and womaned) up and finished the plate. There was no way we were going to eat anything else, so we just hung out outside for a while. \\nI ended up calling a couple other friends to join us, and we moved inside after it got chilly. My friend and I moved onto Bull Dogs (margaritas with an upside down coronita stuck inside), which ended up being a great decision. They were tasty and definitely not a bad deal at $10. My other friend split a fish bowl for $19 and while it was good, the jury is still out as to whether or not there was actually alcohol in it.\\nAll in all the drinks and food were good and it was a fun atmosphere. I could definitely see myself coming back for drinks at the bar with a girl friend, and I am already trying to wrangle up friends for brunch.\\nI  would have ordered another drink (we were there for about 6 hours) but the waitress never came back and by that time we were ready to go. Despite the slow service, the manager was very nice and friendly and checked up on up multiple times. Additionally, the manager ended up comping 2 small and 2 large margaritas at our table, instead of the 4 small margaritas we got from the Foursquare deal which was really nice and saved us $10.\\nAsk for Angela! She is awesome!\\nI will definitely be going back!\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amigos_data.raw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>We need to modify comparitive_emotion_analyzer to tell it where the restaurant name and the text is in the tuple</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restaurant           fear\ttrust negative positive joy   disgust anticip sadness surprise\n"
     ]
    }
   ],
   "source": [
    "def comparative_emotion_analyzer(text_tuples,name_location=1,text_location=2):\n",
    "    print(\"%-20s %1s\\t%1s %1s %1s %1s   %1s %1s %1s %1s\"%(\n",
    "            \"restaurant\",\"fear\",\"trust\",\"negative\",\"positive\",\"joy\",\"disgust\",\"anticip\",\n",
    "            \"sadness\",\"surprise\"))\n",
    "        \n",
    "    for text_tuple in text_tuples:\n",
    "        text = text_tuple[text_location] \n",
    "        result = emotion_analyzer(text)\n",
    "        print(\"%-20s %1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\"%(\n",
    "            text_tuple[name_location][0:20],result['fear'],result['trust'],\n",
    "              result['negative'],result['positive'],result['joy'],result['disgust'],\n",
    "              result['anticipation'],result['sadness'],result['surprise']))\n",
    "        \n",
    "#And test it        \n",
    "comparative_emotion_analyzer(all_snippets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restaurant           fear\ttrust negative positive joy   disgust anticip sadness surprise\n",
      "community            0.00\t0.03\t0.01\t0.05\t0.03\t0.01\t0.02\t0.01\t0.01\n",
      "le monde             0.00\t0.03\t0.01\t0.04\t0.02\t0.00\t0.02\t0.00\t0.01\n",
      "heights              0.00\t0.03\t0.01\t0.04\t0.03\t0.00\t0.03\t0.01\t0.01\n",
      "amigos               0.01\t0.03\t0.01\t0.04\t0.03\t0.01\t0.02\t0.01\t0.01\n"
     ]
    }
   ],
   "source": [
    "restaurant_data = [('community',community_data.raw()),('le monde',le_monde_data.raw())\n",
    "                  ,('heights',heights_data.raw()), ('amigos',amigos_data.raw())]\n",
    "comparative_emotion_analyzer(restaurant_data,0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Simple Analysis: Complexity</h2>\n",
    "<h4>We'll look at four complexity factors</h4>\n",
    "<li>average word length: longer words adds to complexity\n",
    "<li>average sentence length: longer sentences are more complex (unless the text is rambling!)\n",
    "<li>vocabulary: the ratio of unique words used to the total number of words (more variety, more complexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>token:</b> A sequence (or group) of characters of interest. For e.g., in the below analysis, a token = a word\n",
    "<li>Generally: A token is the base unit of analysis</li>\n",
    "<li>So, the first step is to convert text into tokens and nltk text object</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188\n",
      "2597\n"
     ]
    }
   ],
   "source": [
    "#Construct tokens (words/sentences) from the text\n",
    "text = le_monde_data.raw()\n",
    "import nltk\n",
    "from nltk import sent_tokenize,word_tokenize \n",
    "sentences = nltk.Text(sent_tokenize(text))\n",
    "print(len(sentences))\n",
    "words = nltk.Text(word_tokenize(text))\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12332 4 13 0.29110512129380056\n"
     ]
    }
   ],
   "source": [
    "num_chars=len(text)\n",
    "num_words=len(word_tokenize(text))\n",
    "num_sentences=len(sent_tokenize(text))\n",
    "vocab = {x.lower() for x in word_tokenize(text)}\n",
    "print(num_chars,int(num_chars/num_words),int(num_words/num_sentences),(len(vocab)/num_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Functionalize this</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complexity(text):\n",
    "    num_chars=len(text)\n",
    "    num_words=len(word_tokenize(text))\n",
    "    num_sentences=len(sent_tokenize(text))\n",
    "    vocab = {x.lower() for x in word_tokenize(text)}\n",
    "    return len(vocab),int(num_chars/num_words),int(num_words/num_sentences),len(vocab)/num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(756, 4, 13, 0.29110512129380056)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_complexity(le_monde_data.raw())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "community      \t1029.00\t4.00\t16.00\t0.28\n",
      "le monde       \t756.00\t4.00\t13.00\t0.29\n",
      "heights        \t720.00\t4.00\t16.00\t0.28\n",
      "amigos         \t792.00\t4.00\t15.00\t0.24\n"
     ]
    }
   ],
   "source": [
    "for text in restaurant_data:\n",
    "    (vocab,word_size,sent_size,vocab_to_text) = get_complexity(text[1])\n",
    "    print(\"{0:15s}\\t{1:1.2f}\\t{2:1.2f}\\t{3:1.2f}\\t{4:1.2f}\".format(text[0],vocab,word_size,sent_size,vocab_to_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>We could do a word cloud comparison</h4>\n",
    "We'll remove short words and look only at words longer than 6 letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'restaurant_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-1df134b2013a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestaurant_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Remove unwanted words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'restaurant_data' is not defined"
     ]
    }
   ],
   "source": [
    "texts = restaurant_data\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#Remove unwanted words\n",
    "#As we look at the cloud, we can get rid of words that don't make sense by adding them to this variable\n",
    "DELETE_WORDS = []\n",
    "def remove_words(text_string,DELETE_WORDS=DELETE_WORDS):\n",
    "    for word in DELETE_WORDS:\n",
    "        text_string = text_string.replace(word,' ')\n",
    "    return text_string\n",
    "\n",
    "#Remove short words\n",
    "MIN_LENGTH = 0\n",
    "def remove_short_words(text_string,min_length = MIN_LENGTH):\n",
    "    word_list = text_string.split()\n",
    "    for word in word_list:\n",
    "        if len(word) < min_length:\n",
    "            text_string = text_string.replace(' '+word+' ',' ',1)\n",
    "    return text_string\n",
    "\n",
    "\n",
    "#Set up side by side clouds\n",
    "COL_NUM = 2\n",
    "ROW_NUM = 2\n",
    "fig, axes = plt.subplots(ROW_NUM, COL_NUM, figsize=(12,12))\n",
    "\n",
    "for i in range(0,len(texts)):\n",
    "    text_string = remove_words(texts[i][1])\n",
    "    text_string = remove_short_words(text_string)\n",
    "    ax = axes[i%2]\n",
    "    ax = axes[i//2, i%2] #Use this if ROW_NUM >=2\n",
    "    ax.set_title(texts[i][0])\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS,background_color='white',width=1200,height=1000,max_words=20).generate(text_string)\n",
    "    ax.imshow(wordcloud)\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Comparing complexity of restaurant reviews won't get us anything useful</h3>\n",
    "<h3>Let's look at something more useful</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>nltk: Python's natural language toolkit</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>ntlk documentation link:</h3> http://www.nltk.org/api/nltk.html\n",
    "<h3>Commands cheat sheet</h3> https://blogs.princeton.edu/etc/files/2014/03/Text-Analysis-with-NLTK-Cheatsheet.pdf\n",
    "<h3>nltk book</h3>http://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>nltk contains a large corpora of pre-tokenized text</h2>\n",
    "Load it using the command:<p>\n",
    "nltk.download()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Import the corpora</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mgutenberg\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('gutenberg')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/gutenberg\u001b[0m\n\n  Searched in:\n    - '/home/alexei/nltk_data'\n    - '/home/alexei/ds/eme/pro/my_project_env/nltk_data'\n    - '/home/alexei/ds/eme/pro/my_project_env/share/nltk_data'\n    - '/home/alexei/ds/eme/pro/my_project_env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/ds/eme/pro/my_project_env/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ds/eme/pro/my_project_env/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mgutenberg\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('gutenberg')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/gutenberg.zip/gutenberg/\u001b[0m\n\n  Searched in:\n    - '/home/alexei/nltk_data'\n    - '/home/alexei/ds/eme/pro/my_project_env/nltk_data'\n    - '/home/alexei/ds/eme/pro/my_project_env/share/nltk_data'\n    - '/home/alexei/ds/eme/pro/my_project_env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-a6ea6dd70c17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ds/eme/pro/my_project_env/lib/python3.6/site-packages/nltk/book.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Type: 'texts()' or 'sents()' to list the materials.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtext1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgutenberg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"melville-moby_dick.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text1:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ds/eme/pro/my_project_env/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ds/eme/pro/my_project_env/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ds/eme/pro/my_project_env/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ds/eme/pro/my_project_env/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mgutenberg\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('gutenberg')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/gutenberg\u001b[0m\n\n  Searched in:\n    - '/home/alexei/nltk_data'\n    - '/home/alexei/ds/eme/pro/my_project_env/nltk_data'\n    - '/home/alexei/ds/eme/pro/my_project_env/share/nltk_data'\n    - '/home/alexei/ds/eme/pro/my_project_env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Often, a comparitive analysis helps us understand text better</h1>\n",
    "<h2>Let's look at US Presidentinaugural speeches</h2>\n",
    "<h4>Copy the files 2013-Obama.txt and 2017-Trump.txt to the nltk_data/corpora/inaugural directory. nltk_data should be under your home directory</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u001b[0m\u001b[01;34mbin\u001b[0m/         examples.desktop  \u001b[01;36m\"PlayOnLinux's virtual drives\"\u001b[0m@   \u001b[01;34mTemplates\u001b[0m/\r\n",
      " \u001b[01;34mDesktop\u001b[0m/     \u001b[01;34mgoogle-drive\u001b[0m/      \u001b[01;34mPublic\u001b[0m/                          \u001b[01;34mVideos\u001b[0m/\r\n",
      " \u001b[01;34mDocuments\u001b[0m/   mozilla.pdf        \u001b[01;34mscikit_learn_data\u001b[0m/               winehq.key\r\n",
      " \u001b[01;34mDownloads\u001b[0m/   \u001b[01;34mMusic\u001b[0m/             \u001b[01;34mseaborn-data\u001b[0m/\r\n",
      " \u001b[01;34mds\u001b[0m/          \u001b[01;34mPictures\u001b[0m/          \u001b[01;34msnap\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-08ecd99da1a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#nltk.download('inaugural')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnltk_data\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0minaugural\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk_data' is not defined"
     ]
    }
   ],
   "source": [
    "#nltk.download('inaugural')\n",
    "\n",
    "nltk_data/data/inaugural.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inaugural' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-4f370d8b9c01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minaugural\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1861-Lincoln.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'inaugural' is not defined"
     ]
    }
   ],
   "source": [
    "inaugural.raw('1861-Lincoln.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Let's look at the complexity of the speeches by four presidents</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inaugural' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-05f3edcb5c02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m texts = [('trump',inaugural.raw('2017-Trump.txt')),\n\u001b[0m\u001b[1;32m      2\u001b[0m          \u001b[0;34m(\u001b[0m\u001b[0;34m'obama'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minaugural\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2009-Obama.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0minaugural\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2013-Obama.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m          \u001b[0;34m(\u001b[0m\u001b[0;34m'jackson'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minaugural\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1829-Jackson.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0minaugural\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1833-Jackson.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m          ('washington',inaugural.raw('1789-Washington.txt')+inaugural.raw('1793-Washington.txt'))]\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inaugural' is not defined"
     ]
    }
   ],
   "source": [
    "texts = [('trump',inaugural.raw('2017-Trump.txt')),\n",
    "         ('obama',inaugural.raw('2009-Obama.txt')+inaugural.raw('2013-Obama.txt')),\n",
    "         ('jackson',inaugural.raw('1829-Jackson.txt')+inaugural.raw('1833-Jackson.txt')),\n",
    "         ('washington',inaugural.raw('1789-Washington.txt')+inaugural.raw('1793-Washington.txt'))]\n",
    "for text in texts:\n",
    "    (vocab,word_size,sent_size,vocab_to_text) = get_complexity(text[1])\n",
    "    print(\"{0:15s}\\t{1:1.2f}\\t{2:1.2f}\\t{3:1.2f}\\t{4:1.2f}\".format(text[0],vocab,word_size,sent_size,vocab_to_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Analysis over time</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The files are arranged over time so we can analyze how complexity has changed between Washington and Trump</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural\n",
    "sentence_lengths = list()\n",
    "for fileid in inaugural.fileids():\n",
    "    sentence_lengths.append(get_complexity(' '.join(inaugural.words(fileid)))[2])\n",
    "plt.plot(sentence_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>dispersion plots</h1>\n",
    "<h2>Dispersion plots show the relative frequency of words over the text</h2>\n",
    "<h3>Let's see how the frequency of some words has changed over the course of the republic</h3>\n",
    "<h3>That should give us some idea of how the focus of the nation has changed</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text4.dispersion_plot([\"government\", \"citizen\", \"freedom\", \"duties\", \"America\",'independence','God','patriotism'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>We may want to use word stems rather than the part of speect form</h4>\n",
    "<li>For example: patriot, patriotic, patriotism all express roughly the same idea\n",
    "<li>nltk has a stemmer that implements the \"Porter Stemming Algorithm\" (https://tartarus.org/martin/PorterStemmer/)\n",
    "<li>We'll push everything to lowercase as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "text = inaugural.raw()\n",
    "striptext = text.replace('\\n\\n', ' ')\n",
    "striptext = striptext.replace('\\n', ' ')\n",
    "sentences = sent_tokenize(striptext)\n",
    "words = word_tokenize(striptext)\n",
    "text = nltk.Text([p_stemmer.stem(i).lower() for i in words])\n",
    "text.dispersion_plot([\"govern\", \"citizen\", \"free\", \"america\",'independ','god','patriot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Weighted word analysis using Vader</h2>\n",
    "<h4>Vader contains a list of 7500 features weighted by how positive or negative they are</h4>\n",
    "<h4>It uses these features to calculate stats on how positive, negative and neutral a passage is</h4>\n",
    "<h4>And combines these results to give a compound sentiment (higher = more positive) for the passage</h4>\n",
    "<h4>Human trained on twitter data and generally considered good for informal communication</h4>\n",
    "<h4>10 humans rated each feature in each tweet in context from -4 to +4</h4>\n",
    "<h4>Calculates the sentiment in a sentence using word order analysis</h4>\n",
    "<li>\"marginally good\" will get a lower positive score than \"extremely good\"\n",
    "<h4>Computes a \"compound\" score based on heuristics (between -1 and +1)</h4>\n",
    "<h4>Includes sentiment of emoticons, punctuation, and other 'social media' lexicon elements</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['pos','neg','neu','compound']\n",
    "texts = restaurant_data\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for i in range(len(texts)):\n",
    "    name = texts[i][0]\n",
    "    sentences = sent_tokenize(texts[i][1])\n",
    "    pos=compound=neu=neg=0\n",
    "    for sentence in sentences:\n",
    "        vs = analyzer.polarity_scores(sentence)\n",
    "        pos+=vs['pos']/(len(sentences))\n",
    "        compound+=vs['compound']/(len(sentences))\n",
    "        neu+=vs['neu']/(len(sentences))\n",
    "        neg+=vs['neg']/(len(sentences))\n",
    "    print(name,pos,neg,neu,compound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>And functionalize this as well</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vader_comparison(texts):\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    headers = ['pos','neg','neu','compound']\n",
    "    print(\"Name\\t\",'  pos\\t','neg\\t','neu\\t','compound')\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    for i in range(len(texts)):\n",
    "        name = texts[i][0]\n",
    "        sentences = sent_tokenize(texts[i][1])\n",
    "        pos=compound=neu=neg=0\n",
    "        for sentence in sentences:\n",
    "            vs = analyzer.polarity_scores(sentence)\n",
    "            pos+=vs['pos']/(len(sentences))\n",
    "            compound+=vs['compound']/(len(sentences))\n",
    "            neu+=vs['neu']/(len(sentences))\n",
    "            neg+=vs['neg']/(len(sentences))\n",
    "        print('%-10s'%name,'%1.2f\\t'%pos,'%1.2f\\t'%neg,'%1.2f\\t'%neu,'%1.2f\\t'%compound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_comparison(restaurant_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>Named Entities</h2>\n",
    "<h4>People, places, organizations</h4>\n",
    "Named entities are often the subject of sentiments so identifying them can be very useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Named entity detection is based on Part-of-speech tagging of words and chunks (groups of words)</h4>\n",
    "<li>Start with sentences (using a sentence tokenizer)\n",
    "<li>tokenize words in each sentence\n",
    "<li>chunk them. ne_chunk identifies likely chunked candidates (ne = named entity)\n",
    "<li>Finally build chunks using nltk's guess on what members of chunk represent (people, place, organization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en={}\n",
    "try:\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = sent_detector.tokenize(community_data.raw().strip())\n",
    "    for sentence in sentences:\n",
    "            tokenized = nltk.word_tokenize(sentence)\n",
    "            tagged = nltk.pos_tag(tokenized)\n",
    "            chunked = nltk.ne_chunk(tagged)\n",
    "            for tree in chunked:\n",
    "                if hasattr(tree, 'label'):\n",
    "                    ne = ' '.join(c[0] for c in tree.leaves())\n",
    "                    en[ne] = [tree.label(), ' '.join(c[1] for c in tree.leaves())]\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Assuming we've done a good job of identifying named entities, we can get an affect score on entities</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_sents = list()\n",
    "i=0\n",
    "for sentence in sentences:\n",
    "    if 'service' in sentence:\n",
    "        i+=1\n",
    "        meaningful_sents.append((i,sentence))\n",
    "\n",
    "vader_comparison(meaningful_sents)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>We could also develop a affect calculator for common terms in our domain (e.g., food items)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_affect(text,word,lower=False):\n",
    "    import nltk\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = sent_detector.tokenize(text.strip())\n",
    "    sentence_count = 0\n",
    "    running_total = 0\n",
    "    for sentence in sentences:\n",
    "        if lower: sentence = sentence.lower()\n",
    "        if word in sentence:\n",
    "            vs = analyzer.polarity_scores(sentence) \n",
    "            running_total += vs['compound']\n",
    "            sentence_count += 1\n",
    "    if sentence_count == 0: return 0\n",
    "    return running_total/sentence_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_affect(community_data.raw(),'service',True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>The nltk function concordance returns text fragments around a word</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.Text(community_data.words()).concordance('service',100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text summarization</h2>\n",
    "<h4>Text summarization is useful because you can generate a short summary of a large piece of text automatically</h4>\n",
    "<h4>Then, these summaries can serve as an input into a topic analyzer to figure out what the main topic of the text is</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A naive form of summarization is to identify the most frequent words in a piece of text and use the occurrence of these words in sentences to rate the importance of a sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>First the imports</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from collections import OrderedDict\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Then prep the text. Get did of end of line chars</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = community_data.raw()\n",
    "summary_sentences = []\n",
    "candidate_sentences = {}\n",
    "candidate_sentence_counts = {}\n",
    "striptext = text.replace('\\n\\n', ' ')\n",
    "striptext = striptext.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Construct a list of words after getting rid of unimportant ones and numbers</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(striptext)\n",
    "lowercase_words = [word.lower() for word in words\n",
    "                  if word not in stopwords.words() and word.isalpha()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Construct word frequencies and choose the most common n (20)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = FreqDist(lowercase_words)\n",
    "most_frequent_words = FreqDist(lowercase_words).most_common(20)\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(most_frequent_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>lowercase the sentences</h4>\n",
    "candidate_sentences is a dictionary with the original sentence as the key, and its lowercase version as the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(striptext)\n",
    "for sentence in sentences:\n",
    "    candidate_sentences[sentence] = sentence.lower()\n",
    "candidate_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for long, short in candidate_sentences.items():\n",
    "    count = 0\n",
    "    for freq_word, frequency_score in most_frequent_words:\n",
    "        if freq_word in short:\n",
    "            count += frequency_score\n",
    "            candidate_sentence_counts[long] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OrderedDict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7337dc068114>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m sorted_sentences = OrderedDict(sorted(\n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0mcandidate_sentence_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     reverse = True)[:4])\n\u001b[1;32m      5\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OrderedDict' is not defined"
     ]
    }
   ],
   "source": [
    "sorted_sentences = OrderedDict(sorted(\n",
    "                    candidate_sentence_counts.items(),\n",
    "                    key = lambda x: x[0],\n",
    "                    reverse = True)[:4])\n",
    "pp.pprint(sorted_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Packaging all this into a function</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_naive_summary(text):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    from nltk.probability import FreqDist\n",
    "    from nltk.corpus import stopwords\n",
    "    from collections import OrderedDict\n",
    "    summary_sentences = []\n",
    "    candidate_sentences = {}\n",
    "    candidate_sentence_counts = {}\n",
    "    striptext = text.replace('\\n\\n', ' ')\n",
    "    striptext = striptext.replace('\\n', ' ')\n",
    "    words = word_tokenize(striptext)\n",
    "    lowercase_words = [word.lower() for word in words\n",
    "                      if word not in stopwords.words() and word.isalpha()]\n",
    "    word_frequencies = FreqDist(lowercase_words)\n",
    "    most_frequent_words = FreqDist(lowercase_words).most_common(20)\n",
    "    sentences = sent_tokenize(striptext)\n",
    "    for sentence in sentences:\n",
    "        candidate_sentences[sentence] = sentence.lower()\n",
    "    for long, short in candidate_sentences.items():\n",
    "        count = 0\n",
    "        for freq_word, frequency_score in most_frequent_words:\n",
    "            if freq_word in short:\n",
    "                count += frequency_score\n",
    "                candidate_sentence_counts[long] = count   \n",
    "    sorted_sentences = OrderedDict(sorted(\n",
    "                        candidate_sentence_counts.items(),\n",
    "                        key = lambda x: x[1],\n",
    "                        reverse = True)[:4])\n",
    "    return sorted_sentences   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = '\\n'.join(build_naive_summary(community_data.raw()))\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = '\\n'.join(build_naive_summary(le_monde_data.raw()))\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>We can summarize George Washington's first inaugural speech<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_naive_summary(inaugural.raw('1789-Washington.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>gensim: another text summarizer</h3>\n",
    "Gensim uses a network with sentences as nodes and 'lexical similarity' as weights on the arcs between nodes<p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk import sent_tokenize,word_tokenize \n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "community_root = \"data/community\"\n",
    "le_monde_root = \"data/le_monde\"\n",
    "community_files = \"community.*\"\n",
    "le_monde_files = \"le_monde.*\"\n",
    "heights_root = \"data/heights\"\n",
    "heights_files = \"heights.*\"\n",
    "amigos_root = \"data/amigos\"\n",
    "amigos_files = \"amigos.*\"\n",
    "community_data = PlaintextCorpusReader(community_root,community_files)\n",
    "le_monde_data = PlaintextCorpusReader(le_monde_root,le_monde_files)\n",
    "heights_data = PlaintextCorpusReader(heights_root,heights_files)\n",
    "amigos_data = PlaintextCorpusReader(amigos_root,amigos_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(community_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = community_data.raw()\n",
    "summary_sentences = []\n",
    "candidate_sentences = {}\n",
    "candidate_sentence_counts = {}\n",
    "striptext = text.replace('\\n\\n', ' ')\n",
    "striptext = striptext.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim.summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = gensim.summarization.summarize(striptext, word_count=100) \n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gensim.summarization.keywords(striptext,words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = '\\n'.join(build_naive_summary(community_data.raw()))\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = le_monde_data.raw()\n",
    "summary_sentences = []\n",
    "candidate_sentences = {}\n",
    "candidate_sentence_counts = {}\n",
    "striptext = text.replace('\\n\\n', ' ')\n",
    "striptext = striptext.replace('\\n', ' ')\n",
    "summary = gensim.summarization.summarize(striptext, word_count=100) \n",
    "print(summary)\n",
    "#print(gensim.summarization.keywords(striptext,words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Topic modeling</h1>\n",
    "<h4>The goal of topic modeling is to identify the major concepts underlying a piece of text</h4>\n",
    "<h4>Topic modeling uses \"Unsupervised Learning\". No apriori knowledge is necessary\n",
    "<li>Though it is helpful in cleaning up results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>LDA: Latent Dirichlet Allocation Model</h3>\n",
    "<li>Identifies potential topics using pruning techniques like 'upward closure'\n",
    "<li>Computes conditional probabilities for topic word sets\n",
    "<li>Identifies the most likely topics\n",
    "<li>Does this over multiple passes probabilistically picking topics in each pass\n",
    "<li>Good intuitive explanation: http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Prepare the text</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = PlaintextCorpusReader(\"data/\",\"Nikon_coolpix_4300.txt\").raw()\n",
    "striptext = text.replace('\\n\\n', ' ')\n",
    "striptext = striptext.replace('\\n', ' ')\n",
    "sentences = sent_tokenize(striptext)\n",
    "#words = word_tokenize(striptext)\n",
    "#tokenize each sentence into word tokens\n",
    "texts = [[word for word in sentence.lower().split()\n",
    "        if word not in STOPWORDS and word.isalnum()]\n",
    "        for sentence in sentences]\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Create a (word,frequency) dictionary for each word in the text</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts) #(word_id,frequency) pairs\n",
    "corpus = [dictionary.doc2bow(text) for text in texts] #(word_id,freq) pairs by sentence\n",
    "#print(dictionary.token2id)\n",
    "#print(dictionary.keys())\n",
    "#print(corpus[9])\n",
    "#print(texts[9])\n",
    "#print(dictionary[73])\n",
    "#dictionary[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Do the LDA</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Parameters:</h4>\n",
    "<li>Number of topics: The number of topics you want generated. The larger the document, the more the desirable topics\n",
    "<li>Passes: The LDA model makes through the document. More passes, slower analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set parameters\n",
    "num_topics = 5 #The number of topics that should be generated\n",
    "passes = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LdaModel(corpus,\n",
    "              id2word=dictionary,\n",
    "              num_topics=num_topics,\n",
    "              passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>See results</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(lda.print_topics(num_words=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Matching topics to documents</h2>\n",
    "<h3>Sort topics by probability</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>We're using sentences as documents here, so this is less than ideal</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "lda.get_document_topics(corpus[0],minimum_probability=0.05,per_word_topics=False)\n",
    "sorted(lda.get_document_topics(corpus[0],minimum_probability=0,per_word_topics=False),key=itemgetter(1),reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Making sense of the topics</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Draw wordclouds</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_wordcloud(lda,topicnum,min_size=0,STOPWORDS=[]):\n",
    "    word_list=[]\n",
    "    prob_total = 0\n",
    "    for word,prob in lda.show_topic(topicnum,topn=50):\n",
    "        prob_total +=prob\n",
    "    for word,prob in lda.show_topic(topicnum,topn=50):\n",
    "        if word in STOPWORDS or  len(word) < min_size:\n",
    "            continue\n",
    "        freq = int(prob/prob_total*1000)\n",
    "        alist=[word]\n",
    "        word_list.extend(alist*freq)\n",
    "\n",
    "    from wordcloud import WordCloud, STOPWORDS\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    text = ' '.join(word_list)\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS,background_color='white',width=3000,height=3000).generate(' '.join(word_list))\n",
    "\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_wordcloud(lda,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Roughly,</h4>\n",
    "<li>lda looks for candidate topics assuming that there are many such candidates\n",
    "<li>looks for words related to the candidate topics\n",
    "<li>assign probablilites to those words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Let's look at Presidential addresses to see what sorts of topics emerge from there</h3>\n",
    "<li>Each document will be analyzed for topic</li>\n",
    "<li>The corpus will consist of 58 documents, one per presidential address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "REMOVE_WORDS = {'shall','generally','spirit','country','people','nation','nations','great','better'}\n",
    "#Create a word dictionary (id, word)\n",
    "texts = [[word for word in sentence.lower().split()\n",
    "        if word not in STOPWORDS and word not in REMOVE_WORDS and word.isalnum()]\n",
    "        for sentence in sentences]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "#Create a corpus of documents\n",
    "text_list = list()\n",
    "for fileid in inaugural.fileids():\n",
    "    text = inaugural.words(fileid)\n",
    "    doc=list()\n",
    "    for word in text:\n",
    "        if word in STOPWORDS or word in REMOVE_WORDS or not word.isalpha() or len(word) <5:\n",
    "            continue\n",
    "        doc.append(word)\n",
    "    text_list.append(doc)\n",
    "by_address_corpus = [dictionary.doc2bow(text) for text in text_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create the model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(by_address_corpus,\n",
    "              id2word=dictionary,\n",
    "              num_topics=20,\n",
    "              passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(lda.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>We can now compare presidential addresses by topic</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(by_address_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "sorted(lda.get_document_topics(by_address_corpus[0],minimum_probability=0,per_word_topics=False),key=itemgetter(1),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_wordcloud(lda,18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda.show_topic(12,topn=5))\n",
    "print(lda.show_topic(18,topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Similarity</h1>\n",
    "<h2>Given a corpus of documents, when a new document arrives, find the document that is the most similar</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = [community_data,le_monde_data,amigos_data,heights_data]\n",
    "all_text = community_data.raw() + le_monde_data.raw() + amigos_data.raw() + heights_data.raw()\n",
    "\n",
    "documents = [doc.raw() for doc in doc_list]\n",
    "texts = [[word for word in document.lower().split()\n",
    "        if word not in STOPWORDS and word.isalnum()]\n",
    "        for document in documents]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.similarities.docsim import Similarity\n",
    "from gensim import corpora, models, similarities\n",
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)\n",
    "doc = \"\"\"\n",
    "Many, many years ago, I used to frequent this place for their amazing french toast. \n",
    "It's been a while since then and I've been hesitant to review a place I haven't been to in 7-8 years... \n",
    "but I passed by French Roast and, feeling nostalgic, decided to go back.\n",
    "\n",
    "It was a great decision.\n",
    "\n",
    "Their Bloody Mary is fantastic and includes bacon (which was perfectly cooked!!), olives, \n",
    "cucumber, and celery. The Irish coffee is also excellent, even without the cream which is what I ordered.\n",
    "\n",
    "Great food, great drinks, a great ambiance that is casual yet familiar like a tiny little French cafe. \n",
    "I highly recommend coming here, and will be back whenever I'm in the area next.\n",
    "\n",
    "Juan, the bartender, is great!! One of the best in any brunch spot in the city, by far.\n",
    "\"\"\"\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow]\n",
    "index = similarities.MatrixSimilarity(lsi[corpus])\n",
    "sims = index[vec_lsi]\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=\"\"\"\n",
    "I went to Mexican Festival Restaurant for Cinco De Mayo because I had been there years \n",
    "prior and had such a good experience. This time wasn't so good. The food was just \n",
    "mediocre and it wasn't hot when it was brought to our table. They brought my friends food out \n",
    "10 minutes before everyone else and it took forever to get drinks. We let it slide because the place was \n",
    "packed with people and it was Cinco De Mayo. Also, the margaritas we had were slamming! Pure tequila. \n",
    "\n",
    "But then things took a turn for the worst. As I went to get something out of my purse which was on \n",
    "the back of my chair, I looked down and saw a huge water bug. I had to warn the lady next to me because \n",
    "it was so close to her chair. We called the waitress over and someone came with a broom and a dustpan and \n",
    "swept it away like it was an everyday experience. No one seemed phased.\n",
    "\n",
    "Even though our waitress was very nice, I do not think we will be returning to Mexican Festival again. \n",
    "It seems the restaurant is a shadow of its former self.\n",
    "\"\"\"\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow]\n",
    "index = similarities.MatrixSimilarity(lsi[corpus])\n",
    "sims = index[vec_lsi]\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
