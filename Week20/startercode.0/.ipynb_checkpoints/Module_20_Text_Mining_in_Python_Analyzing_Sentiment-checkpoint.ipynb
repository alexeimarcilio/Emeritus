{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Module 20 -  Text Mining in Python -- Analyzing Sentiment\n",
    "\n",
    "\n",
    "**_Author: Jessica Cervi_**\n",
    "\n",
    "**Expected time = 3 hours**\n",
    "\n",
    "**Total points = 120 points**\n",
    "\n",
    "## Assignment Overview\n",
    "\n",
    "\n",
    "Sentiment analysis  falls into the broad category of text classification tasks where you are supplied with a phrase, or a list of phrases and your classifier is supposed to tell if the sentiment behind that is positive or negative. Consider for example the example below:\n",
    "\n",
    "Consider the following phrases:\n",
    "- \"Titanic is a great movie.\"\n",
    "- \"Titanic is not a great movie.\"\n",
    "\n",
    "The phrases correspond to short movie reviews, and each one of them conveys different sentiments. For example, the first phrase denotes positive sentiment about the film Titanic while the second one treats the movie as not so great (negative sentiment). In this assignment, you'll work with a data set to tokenize text, examine word frequencies, train and apply a sentiment classifer. \n",
    "\n",
    "This assignment is designed to build your familiarity and comfort coding in Python while also helping you review key topics from each module. As you progress through the assignment, answers will get increasingly complex. It is important that you adopt a data scientist's mindset when completing this assignment. **Remember to run your code from each cell before submitting your assignment.** Running your code beforehand will notify you of errors and give you a chance to fix your errors before submitting. You should view your Vocareum submission as if you are delivering a final project to your manager or client. \n",
    "\n",
    "***Vocareum Tips***\n",
    "- Do not add arguments or options to functions unless you are specifically asked to. This will cause an error in Vocareum.\n",
    "- Do not use a library unless you are expicitly asked to in the question. \n",
    "- You can download the Grading Report after submitting the assignment. This will include feedback and hints on incorrect questions. \n",
    "\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Use `nltk` to tokenize text\n",
    "- Use `nltk` to examine word frequencies\n",
    "- Use `scikitlearn` and `LogisticRegression` to train a  sentiment classifier\n",
    "- Apply our sentiment classifier to Stephen King reviews\n",
    "- Use `textblob` to examine *sentiment* and *polarity* of a text\n",
    "- Remove **stopwords** and punctuation from text\n",
    "- Compute **complexity** of text using Lexical Diversity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/king_clean.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>review</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stephen King</td>\n",
       "      <td>2011-11-13</td>\n",
       "      <td>By ERROL MORRISNOV. 10, 2011 In all of Stephen...</td>\n",
       "      <td>11/22/63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stephen King</td>\n",
       "      <td>2011-10-31</td>\n",
       "      <td>By JANET MASLINOCT. 30, 2011 Stephen King’s la...</td>\n",
       "      <td>11/22/63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stephen King</td>\n",
       "      <td>2004-01-04</td>\n",
       "      <td>By ANDREW O'HEHIRJAN. 4, 2004 Wolves of the Ca...</td>\n",
       "      <td>Wolves of the Calla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stephen King</td>\n",
       "      <td>1993-10-24</td>\n",
       "      <td>By RICHARD E. NICHOLLSOCT. 24, 1993           ...</td>\n",
       "      <td>Nightmares and Dreamscapes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stephen King</td>\n",
       "      <td>2001-11-04</td>\n",
       "      <td>By MARY ELIZABETH WILLIAMSNOV. 4, 2001 BLACK H...</td>\n",
       "      <td>Black House</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         author        date  \\\n",
       "0  Stephen King  2011-11-13   \n",
       "1  Stephen King  2011-10-31   \n",
       "2  Stephen King  2004-01-04   \n",
       "3  Stephen King  1993-10-24   \n",
       "4  Stephen King  2001-11-04   \n",
       "\n",
       "                                              review  \\\n",
       "0  By ERROL MORRISNOV. 10, 2011 In all of Stephen...   \n",
       "1  By JANET MASLINOCT. 30, 2011 Stephen King’s la...   \n",
       "2  By ANDREW O'HEHIRJAN. 4, 2004 Wolves of the Ca...   \n",
       "3  By RICHARD E. NICHOLLSOCT. 24, 1993           ...   \n",
       "4  By MARY ELIZABETH WILLIAMSNOV. 4, 2001 BLACK H...   \n",
       "\n",
       "                        title  \n",
       "0                    11/22/63  \n",
       "1                    11/22/63  \n",
       "2         Wolves of the Calla  \n",
       "3  Nightmares and Dreamscapes  \n",
       "4                 Black House  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q1'></a>\n",
    "\n",
    "### Question 1:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "To get started, we want to focus on a straightforward approach to sentiment analysis similar to what we saw in the lectures.  Before we can count **positive** and **negative** words however, we need to **tokenize** our reviews.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'By ERROL MORRISNOV. 10, 2011 In all of Stephen King’s work there is an admixture of the ordinary and the supernatural — call it the weird quotidian. In his new novel, “11/22/63,” it is a rabbit hole into the past that pops up in Lisbon Falls, a woebegone corner of Maine. On one end is 2011. An unpopular diner has finally been bought out by L. L. Bean. The diner — and the time portal inside it — may last a few more weeks in the footprint of a burned textile mill. On the other end is America under'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][0][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Use the nltk function `word_tokenize` to create a list of tokens from the `review_0` string defined below. Assign the resultant list of tokens to the `ans_1` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "review_0 = df['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct tokens (words/sentences) from the text\n",
    "# text = le_monde_data.raw()\n",
    "# import nltk\n",
    "# from nltk import sent_tokenize,word_tokenize \n",
    "# sentences = nltk.Text(sent_tokenize(text))\n",
    "# print(len(sentences))\n",
    "# words = nltk.Text(word_tokenize(text))\n",
    "# print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "ans1 = word_tokenize(review_0)\n",
    "\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['By',\n",
       " 'ERROL',\n",
       " 'MORRISNOV',\n",
       " '.',\n",
       " '10',\n",
       " ',',\n",
       " '2011',\n",
       " 'In',\n",
       " 'all',\n",
       " 'of',\n",
       " 'Stephen',\n",
       " 'King',\n",
       " '’',\n",
       " 's',\n",
       " 'work',\n",
       " 'there',\n",
       " 'is',\n",
       " 'an',\n",
       " 'admixture',\n",
       " 'of',\n",
       " 'the',\n",
       " 'ordinary',\n",
       " 'and',\n",
       " 'the',\n",
       " 'supernatural',\n",
       " '—',\n",
       " 'call',\n",
       " 'it',\n",
       " 'the',\n",
       " 'weird',\n",
       " 'quotidian',\n",
       " '.',\n",
       " 'In',\n",
       " 'his',\n",
       " 'new',\n",
       " 'novel',\n",
       " ',',\n",
       " '“',\n",
       " '11/22/63',\n",
       " ',',\n",
       " '”',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'rabbit',\n",
       " 'hole',\n",
       " 'into',\n",
       " 'the',\n",
       " 'past',\n",
       " 'that',\n",
       " 'pops',\n",
       " 'up',\n",
       " 'in',\n",
       " 'Lisbon',\n",
       " 'Falls',\n",
       " ',',\n",
       " 'a',\n",
       " 'woebegone',\n",
       " 'corner',\n",
       " 'of',\n",
       " 'Maine',\n",
       " '.',\n",
       " 'On',\n",
       " 'one',\n",
       " 'end',\n",
       " 'is',\n",
       " '2011',\n",
       " '.',\n",
       " 'An',\n",
       " 'unpopular',\n",
       " 'diner',\n",
       " 'has',\n",
       " 'finally',\n",
       " 'been',\n",
       " 'bought',\n",
       " 'out',\n",
       " 'by',\n",
       " 'L.',\n",
       " 'L.',\n",
       " 'Bean',\n",
       " '.',\n",
       " 'The',\n",
       " 'diner',\n",
       " '—',\n",
       " 'and',\n",
       " 'the',\n",
       " 'time',\n",
       " 'portal',\n",
       " 'inside',\n",
       " 'it',\n",
       " '—',\n",
       " 'may',\n",
       " 'last',\n",
       " 'a',\n",
       " 'few',\n",
       " 'more',\n",
       " 'weeks',\n",
       " 'in',\n",
       " 'the',\n",
       " 'footprint',\n",
       " 'of',\n",
       " 'a',\n",
       " 'burned',\n",
       " 'textile',\n",
       " 'mill',\n",
       " '.',\n",
       " 'On',\n",
       " 'the',\n",
       " 'other',\n",
       " 'end',\n",
       " 'is',\n",
       " 'America',\n",
       " 'under',\n",
       " 'Eisenhower',\n",
       " '.',\n",
       " 'The',\n",
       " 'mill',\n",
       " 'churns',\n",
       " 'out',\n",
       " 'white',\n",
       " 'smoke',\n",
       " '.',\n",
       " '“',\n",
       " 'Vertigo',\n",
       " '”',\n",
       " 'is',\n",
       " 'showing',\n",
       " 'at',\n",
       " 'the',\n",
       " 'outdoor',\n",
       " 'movie',\n",
       " 'theater',\n",
       " '—',\n",
       " 'on',\n",
       " 'its',\n",
       " 'first',\n",
       " 'run',\n",
       " '.',\n",
       " 'The',\n",
       " 'Kennebec',\n",
       " 'Fruit',\n",
       " 'Company',\n",
       " 'isn',\n",
       " '’',\n",
       " 't',\n",
       " 'a',\n",
       " 'curio',\n",
       " 'for',\n",
       " 'tourists',\n",
       " ';',\n",
       " 'it',\n",
       " 'sells',\n",
       " 'oranges',\n",
       " '.',\n",
       " 'And',\n",
       " 'John',\n",
       " 'Kennedy',\n",
       " ',',\n",
       " 'the',\n",
       " 'young',\n",
       " 'senator',\n",
       " 'from',\n",
       " 'Massachusetts',\n",
       " ',',\n",
       " 'is',\n",
       " 'still',\n",
       " 'alive',\n",
       " '.',\n",
       " 'The',\n",
       " 'rules',\n",
       " 'of',\n",
       " 'the',\n",
       " 'rabbit',\n",
       " 'hole',\n",
       " 'into',\n",
       " 'the',\n",
       " 'past',\n",
       " 'are',\n",
       " 'outlined',\n",
       " 'in',\n",
       " 'the',\n",
       " 'first',\n",
       " 'pages',\n",
       " 'of',\n",
       " 'the',\n",
       " 'novel',\n",
       " '.',\n",
       " 'Al',\n",
       " 'Templeton',\n",
       " ',',\n",
       " 'the',\n",
       " 'owner',\n",
       " 'of',\n",
       " 'the',\n",
       " 'diner',\n",
       " ',',\n",
       " 'explains',\n",
       " 'them',\n",
       " 'to',\n",
       " 'Jake',\n",
       " 'Epping',\n",
       " ',',\n",
       " 'an',\n",
       " 'English',\n",
       " 'teacher',\n",
       " 'at',\n",
       " 'the',\n",
       " 'local',\n",
       " 'high',\n",
       " 'school',\n",
       " '.',\n",
       " 'Walk',\n",
       " 'to',\n",
       " 'the',\n",
       " 'back',\n",
       " 'of',\n",
       " 'the',\n",
       " 'pantry',\n",
       " '.',\n",
       " 'Mind',\n",
       " 'the',\n",
       " '60-watt',\n",
       " 'bulb',\n",
       " 'overhead',\n",
       " '.',\n",
       " 'Expect',\n",
       " 'the',\n",
       " 'smell',\n",
       " 'of',\n",
       " 'sulfur',\n",
       " '.',\n",
       " 'And',\n",
       " 'keep',\n",
       " 'walking',\n",
       " 'until',\n",
       " 'you',\n",
       " 'feel',\n",
       " 'your',\n",
       " 'foot',\n",
       " 'fall',\n",
       " '.',\n",
       " 'Suddenly',\n",
       " 'you',\n",
       " '’',\n",
       " 're',\n",
       " 'back',\n",
       " 'on',\n",
       " 'Sept.',\n",
       " '9',\n",
       " ',',\n",
       " '1958',\n",
       " '.',\n",
       " 'It',\n",
       " '’',\n",
       " 's',\n",
       " '11:58',\n",
       " 'a.m',\n",
       " '.',\n",
       " 'There',\n",
       " 'are',\n",
       " ',',\n",
       " 'Al',\n",
       " 'says',\n",
       " ',',\n",
       " 'only',\n",
       " 'two',\n",
       " 'conditions',\n",
       " '.',\n",
       " 'One',\n",
       " ',',\n",
       " 'it',\n",
       " '’',\n",
       " 's',\n",
       " 'not',\n",
       " 'a',\n",
       " 'one-way',\n",
       " 'trip',\n",
       " '.',\n",
       " 'It',\n",
       " 'doesn',\n",
       " '’',\n",
       " 't',\n",
       " 'have',\n",
       " 'to',\n",
       " 'be',\n",
       " '.',\n",
       " 'But',\n",
       " 'when',\n",
       " 'you',\n",
       " 'return',\n",
       " ',',\n",
       " 'no',\n",
       " 'matter',\n",
       " 'how',\n",
       " 'long',\n",
       " 'you',\n",
       " '’',\n",
       " 've',\n",
       " 'stayed',\n",
       " 'in',\n",
       " 'the',\n",
       " 'past',\n",
       " '—',\n",
       " 'two',\n",
       " 'days',\n",
       " ',',\n",
       " 'five',\n",
       " 'years',\n",
       " ',',\n",
       " 'whatever',\n",
       " '—',\n",
       " 'only',\n",
       " 'two',\n",
       " 'minutes',\n",
       " 'have',\n",
       " 'gone',\n",
       " 'by',\n",
       " 'in',\n",
       " 'the',\n",
       " 'present',\n",
       " '.',\n",
       " 'Two',\n",
       " ',',\n",
       " 'each',\n",
       " 'time',\n",
       " 'you',\n",
       " 'go',\n",
       " 'back',\n",
       " 'to',\n",
       " 'the',\n",
       " 'past',\n",
       " ',',\n",
       " 'there',\n",
       " 'is',\n",
       " 'a',\n",
       " 'reset',\n",
       " '.',\n",
       " 'Like',\n",
       " 'a',\n",
       " 'Magic',\n",
       " 'Slate',\n",
       " '.',\n",
       " 'It',\n",
       " '’',\n",
       " 's',\n",
       " '11:58',\n",
       " 'a.m.',\n",
       " ',',\n",
       " 'and',\n",
       " 'everything',\n",
       " 'you',\n",
       " 'did',\n",
       " 'on',\n",
       " 'your',\n",
       " 'previous',\n",
       " 'trip',\n",
       " 'has',\n",
       " 'been',\n",
       " 'erased',\n",
       " '.',\n",
       " 'With',\n",
       " 'that',\n",
       " ',',\n",
       " 'King',\n",
       " 'dispenses',\n",
       " 'with',\n",
       " 'many',\n",
       " 'of',\n",
       " 'the',\n",
       " 'mechanics',\n",
       " 'of',\n",
       " 'time-travel',\n",
       " '—',\n",
       " 'and',\n",
       " 'thank',\n",
       " 'God',\n",
       " 'for',\n",
       " 'it',\n",
       " '.',\n",
       " 'There',\n",
       " 'is',\n",
       " 'no',\n",
       " 'extended',\n",
       " 'discussion',\n",
       " 'of',\n",
       " 'the',\n",
       " '“',\n",
       " 'grandfather',\n",
       " 'paradox.',\n",
       " '”',\n",
       " '(',\n",
       " '“',\n",
       " 'What',\n",
       " 'if',\n",
       " 'you',\n",
       " 'killed',\n",
       " 'your',\n",
       " 'grandfather',\n",
       " '?',\n",
       " '”',\n",
       " '“',\n",
       " 'Why',\n",
       " 'on',\n",
       " 'earth',\n",
       " 'would',\n",
       " 'you',\n",
       " 'do',\n",
       " 'that',\n",
       " '?',\n",
       " '”',\n",
       " ')',\n",
       " 'The',\n",
       " 'rules',\n",
       " 'are',\n",
       " 'simple',\n",
       " '.',\n",
       " 'There',\n",
       " 'is',\n",
       " 'a',\n",
       " 'reason',\n",
       " 'for',\n",
       " 'this',\n",
       " ':',\n",
       " 'King',\n",
       " 'is',\n",
       " 'after',\n",
       " 'something',\n",
       " 'bigger',\n",
       " '.',\n",
       " '“',\n",
       " '11/22/63',\n",
       " '”',\n",
       " 'is',\n",
       " 'a',\n",
       " 'meditation',\n",
       " 'on',\n",
       " 'memory',\n",
       " ',',\n",
       " 'love',\n",
       " ',',\n",
       " 'loss',\n",
       " ',',\n",
       " 'free',\n",
       " 'will',\n",
       " 'and',\n",
       " 'necessity',\n",
       " '.',\n",
       " 'It',\n",
       " '’',\n",
       " 's',\n",
       " 'a',\n",
       " 'blunderbuss',\n",
       " 'of',\n",
       " 'a',\n",
       " 'book',\n",
       " ',',\n",
       " 'rife',\n",
       " 'with',\n",
       " 'answers',\n",
       " 'to',\n",
       " 'questions',\n",
       " ':',\n",
       " 'Can',\n",
       " 'one',\n",
       " 'man',\n",
       " 'make',\n",
       " 'a',\n",
       " 'difference',\n",
       " '?',\n",
       " 'Can',\n",
       " 'history',\n",
       " 'be',\n",
       " 'changed',\n",
       " ',',\n",
       " 'or',\n",
       " 'does',\n",
       " 'it',\n",
       " 'snap',\n",
       " 'back',\n",
       " 'on',\n",
       " 'itself',\n",
       " 'like',\n",
       " 'a',\n",
       " 'rubber',\n",
       " 'band',\n",
       " '?',\n",
       " 'Does',\n",
       " 'love',\n",
       " 'conquer',\n",
       " 'all',\n",
       " '?',\n",
       " '(',\n",
       " 'The',\n",
       " 'big',\n",
       " 'stuff',\n",
       " '.',\n",
       " ')',\n",
       " 'Al',\n",
       " '—',\n",
       " 'the',\n",
       " 'scuttlebutt',\n",
       " 'is',\n",
       " 'that',\n",
       " 'he',\n",
       " 'is',\n",
       " 'serving',\n",
       " 'burgers',\n",
       " 'made',\n",
       " 'of',\n",
       " 'dog',\n",
       " ',',\n",
       " 'or',\n",
       " 'cat',\n",
       " '—',\n",
       " 'is',\n",
       " 'dying',\n",
       " 'of',\n",
       " 'lung',\n",
       " 'cancer',\n",
       " '.',\n",
       " 'Coughing',\n",
       " 'up',\n",
       " 'blood',\n",
       " 'into',\n",
       " 'a',\n",
       " 'pile',\n",
       " 'of',\n",
       " 'maxi-pads',\n",
       " '.',\n",
       " 'He',\n",
       " 'enlists',\n",
       " 'Jake',\n",
       " 'to',\n",
       " 'do',\n",
       " 'what',\n",
       " 'he',\n",
       " 'couldn',\n",
       " '’',\n",
       " 't',\n",
       " ':',\n",
       " 'stop',\n",
       " 'Lee',\n",
       " 'Harvey',\n",
       " 'Oswald',\n",
       " '.',\n",
       " 'It',\n",
       " '’',\n",
       " 's',\n",
       " 'a',\n",
       " 'fabulous',\n",
       " 'pitch',\n",
       " '.',\n",
       " '“',\n",
       " 'Save',\n",
       " 'Kennedy',\n",
       " ',',\n",
       " 'save',\n",
       " 'his',\n",
       " 'brother',\n",
       " '.',\n",
       " 'Save',\n",
       " 'Martin',\n",
       " 'Luther',\n",
       " 'King',\n",
       " '.',\n",
       " 'Stop',\n",
       " 'the',\n",
       " 'race',\n",
       " 'riots',\n",
       " '.',\n",
       " 'Stop',\n",
       " 'Vietnam',\n",
       " ',',\n",
       " 'maybe',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'Get',\n",
       " 'rid',\n",
       " 'of',\n",
       " 'one',\n",
       " 'wretched',\n",
       " 'waif',\n",
       " ',',\n",
       " 'buddy',\n",
       " ',',\n",
       " 'and',\n",
       " 'you',\n",
       " 'could',\n",
       " 'save',\n",
       " 'millions',\n",
       " 'of',\n",
       " 'lives.',\n",
       " '”',\n",
       " 'Jake',\n",
       " 'Epping',\n",
       " 'is',\n",
       " 'a',\n",
       " 'burned-out',\n",
       " 'teacher',\n",
       " 'with',\n",
       " 'a',\n",
       " 'seriously',\n",
       " 'alcoholic',\n",
       " 'ex-wife',\n",
       " 'and',\n",
       " 'nothing',\n",
       " 'better',\n",
       " 'to',\n",
       " 'do',\n",
       " 'than',\n",
       " 'disappear',\n",
       " 'into',\n",
       " 'the',\n",
       " 'past',\n",
       " '.',\n",
       " 'The',\n",
       " 'guilt',\n",
       " 'trip',\n",
       " 'works',\n",
       " '.',\n",
       " 'And',\n",
       " 'Epping',\n",
       " 'falls',\n",
       " 'into',\n",
       " 'the',\n",
       " 'past',\n",
       " 'with',\n",
       " 'a',\n",
       " 'new',\n",
       " 'name',\n",
       " ',',\n",
       " 'George',\n",
       " 'T.',\n",
       " 'Amberson',\n",
       " '—',\n",
       " 'as',\n",
       " 'if',\n",
       " 'time-travel',\n",
       " 'required',\n",
       " 'a',\n",
       " 'new',\n",
       " 'identity',\n",
       " '—',\n",
       " 'and',\n",
       " 'a',\n",
       " 'clear',\n",
       " 'mission',\n",
       " '.',\n",
       " 'Correct',\n",
       " 'the',\n",
       " 'past',\n",
       " '.',\n",
       " 'Undo',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'evils',\n",
       " 'of',\n",
       " 'the',\n",
       " '20th',\n",
       " 'century',\n",
       " '.',\n",
       " 'Once',\n",
       " 'in',\n",
       " '1958',\n",
       " ',',\n",
       " 'however',\n",
       " ',',\n",
       " 'Amberson',\n",
       " 'is',\n",
       " 'immediately',\n",
       " 'confronted',\n",
       " 'by',\n",
       " 'a',\n",
       " 'double',\n",
       " 'mystery',\n",
       " ':',\n",
       " 'the',\n",
       " 'mystery',\n",
       " 'of',\n",
       " 'what',\n",
       " 'really',\n",
       " 'happened',\n",
       " 'then',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'mystery',\n",
       " 'of',\n",
       " 'what',\n",
       " 'might',\n",
       " 'be',\n",
       " 'otherwise',\n",
       " '.',\n",
       " 'Before',\n",
       " 'George/Jake',\n",
       " 'can',\n",
       " 'alter',\n",
       " 'the',\n",
       " 'course',\n",
       " 'of',\n",
       " 'history',\n",
       " ',',\n",
       " 'he',\n",
       " 'has',\n",
       " 'to',\n",
       " 'know',\n",
       " 'what',\n",
       " 'actually',\n",
       " 'occurred',\n",
       " '.',\n",
       " 'Was',\n",
       " 'it',\n",
       " 'Oswald',\n",
       " ',',\n",
       " 'shooting',\n",
       " 'from',\n",
       " 'the',\n",
       " 'depository',\n",
       " '?',\n",
       " 'Was',\n",
       " 'it',\n",
       " 'a',\n",
       " 'conspiracy',\n",
       " '?',\n",
       " 'Another',\n",
       " 'shooter',\n",
       " 'on',\n",
       " 'the',\n",
       " 'grassy',\n",
       " 'knoll',\n",
       " '?',\n",
       " 'How',\n",
       " 'about',\n",
       " 'George',\n",
       " 'de',\n",
       " 'Mohrenschildt',\n",
       " ',',\n",
       " 'one',\n",
       " 'persistent',\n",
       " 'minor',\n",
       " 'character',\n",
       " 'in',\n",
       " 'conspiracy',\n",
       " 'thinking',\n",
       " '?',\n",
       " 'They',\n",
       " 'are',\n",
       " 'the',\n",
       " 'nightmare',\n",
       " 'uncertainties',\n",
       " 'of',\n",
       " 'an',\n",
       " 'event',\n",
       " 'that',\n",
       " 'has',\n",
       " 'been',\n",
       " 'over-examined',\n",
       " ',',\n",
       " 'and',\n",
       " 'never',\n",
       " 'understood',\n",
       " '.',\n",
       " 'Jake',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'person',\n",
       " '.',\n",
       " 'He',\n",
       " 'can',\n",
       " 'not',\n",
       " 'kill',\n",
       " 'Oswald',\n",
       " 'without',\n",
       " 'first',\n",
       " 'knowing',\n",
       " 'whether',\n",
       " 'he',\n",
       " 'was',\n",
       " 'the',\n",
       " 'responsible',\n",
       " 'party',\n",
       " ',',\n",
       " 'and',\n",
       " 'a',\n",
       " 'good',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'adventure',\n",
       " 'is',\n",
       " 'the',\n",
       " 'investigation',\n",
       " '.',\n",
       " 'Once',\n",
       " 'in',\n",
       " 'Dallas',\n",
       " ',',\n",
       " 'Amberson',\n",
       " 'has',\n",
       " 'years',\n",
       " 'to',\n",
       " 'get',\n",
       " 'to',\n",
       " 'know',\n",
       " 'Oswald',\n",
       " ',',\n",
       " 'but',\n",
       " 'he',\n",
       " 'can',\n",
       " '’',\n",
       " 't',\n",
       " 'just',\n",
       " 'bust',\n",
       " 'down',\n",
       " 'the',\n",
       " 'door',\n",
       " '.',\n",
       " 'History',\n",
       " 'is',\n",
       " 'fragile',\n",
       " ';',\n",
       " 'he',\n",
       " 'has',\n",
       " 'to',\n",
       " 'peer',\n",
       " 'around',\n",
       " 'corners',\n",
       " '.',\n",
       " 'He',\n",
       " 'buys',\n",
       " 'tape',\n",
       " 'recorders',\n",
       " 'and',\n",
       " 'long-distance',\n",
       " 'listening',\n",
       " 'devices',\n",
       " ',',\n",
       " 'moves',\n",
       " 'into',\n",
       " 'grubby',\n",
       " 'neighborhoods',\n",
       " ',',\n",
       " 'trails',\n",
       " 'Oswald',\n",
       " 'as',\n",
       " 'he',\n",
       " 'stashes',\n",
       " 'his',\n",
       " 'rifle',\n",
       " '.',\n",
       " 'What',\n",
       " 'he',\n",
       " 'learns',\n",
       " 'is',\n",
       " 'no',\n",
       " 'surprise',\n",
       " '.',\n",
       " 'Oswald',\n",
       " 'was',\n",
       " 'unpleasant',\n",
       " 'in',\n",
       " 'ordinary',\n",
       " 'ways',\n",
       " '.',\n",
       " 'Emotional',\n",
       " ',',\n",
       " 'violent',\n",
       " 'with',\n",
       " 'his',\n",
       " 'wife',\n",
       " ',',\n",
       " 'unsure',\n",
       " 'of',\n",
       " 'himself',\n",
       " 'and',\n",
       " 'desperate',\n",
       " 'to',\n",
       " 'change',\n",
       " 'a',\n",
       " 'broken',\n",
       " 'world',\n",
       " '.',\n",
       " 'Did',\n",
       " 'he',\n",
       " 'kill',\n",
       " 'Kennedy',\n",
       " '?',\n",
       " 'It',\n",
       " '’',\n",
       " 's',\n",
       " 'easy',\n",
       " 'to',\n",
       " 'see',\n",
       " 'King',\n",
       " ',',\n",
       " 'the',\n",
       " 'writer',\n",
       " 'and',\n",
       " 'researcher',\n",
       " ',',\n",
       " 'as',\n",
       " 'a',\n",
       " 'fellow',\n",
       " 'time-traveler',\n",
       " ',',\n",
       " 'hopelessly',\n",
       " 'curious',\n",
       " 'about',\n",
       " 'what',\n",
       " 'Oswald',\n",
       " 'might',\n",
       " 'say',\n",
       " 'on',\n",
       " 'tape',\n",
       " 'or',\n",
       " 'reveal',\n",
       " 'while',\n",
       " 'strolling',\n",
       " 'around',\n",
       " 'Fort',\n",
       " 'Worth',\n",
       " '.',\n",
       " 'But',\n",
       " 'the',\n",
       " 'past',\n",
       " ',',\n",
       " 'the',\n",
       " 'novel',\n",
       " 'repeatedly',\n",
       " 'reminds',\n",
       " 'us',\n",
       " ',',\n",
       " 'is',\n",
       " 'obdurate',\n",
       " '.',\n",
       " 'Under',\n",
       " 'interrogation',\n",
       " ',',\n",
       " 'it',\n",
       " 'guards',\n",
       " 'its',\n",
       " 'darkest',\n",
       " 'secrets',\n",
       " '.',\n",
       " 'Weeks',\n",
       " 'before',\n",
       " 'the',\n",
       " '22nd',\n",
       " ',',\n",
       " 'Amberson',\n",
       " 'is',\n",
       " 'living',\n",
       " 'below',\n",
       " 'the',\n",
       " 'Oswalds',\n",
       " ',',\n",
       " 'and',\n",
       " 'he',\n",
       " 'still',\n",
       " 'can',\n",
       " '’',\n",
       " 't',\n",
       " 'be',\n",
       " 'sure',\n",
       " ':',\n",
       " '“',\n",
       " 'I',\n",
       " 'tried',\n",
       " 'the',\n",
       " 'distance',\n",
       " 'mic',\n",
       " ',',\n",
       " 'standing',\n",
       " 'on',\n",
       " 'a',\n",
       " 'chair',\n",
       " 'and',\n",
       " 'holding',\n",
       " 'the',\n",
       " 'Tupperware',\n",
       " 'bowl',\n",
       " 'almost',\n",
       " 'against',\n",
       " 'the',\n",
       " 'ceiling',\n",
       " '.',\n",
       " 'With',\n",
       " 'it',\n",
       " 'I',\n",
       " 'could',\n",
       " 'hear',\n",
       " 'Lee',\n",
       " 'talking',\n",
       " 'and',\n",
       " 'de',\n",
       " 'Mohrenschildt',\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 01",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q2'></a>\n",
    "\n",
    "### Question 2:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "Extending the idea of single tokens, we can generate **bigrams** or sequences of pairs of adjacent tokens from the text.  Note, this is a Python *generator* which allows for lazy evaluation.  These are common concepts to encounter in NLP tasks, feel free to read more on generators [here](https://docs.python.org/3/tutorial/classes.html#generators).  \n",
    "\n",
    "To create a list, we have to wrap the `bigram` function in a `list` constructor. Further, `bigrams` expects a list of tokens as an input rather than a single string.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk import bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alice', 'took'), ('took', 'up'), ('up', 'the'), ('the', 'fan'), ('fan', 'and')]\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "Alice took up the fan and gloves and she kept fanning herself all the\n",
    "time she went on talking. \"Dear, dear! How queer everything is to-day!\n",
    "And yesterday things went on just as usual. _Was_ I the same when I got\n",
    "up this morning? But if I'm not the same, the next question is, 'Who in\n",
    "the world am I?' Ah, _that's_ the great puzzle!\"\n",
    "'''\n",
    "tokens = word_tokenize(text)\n",
    "bi_tokens = list(bigrams(tokens))\n",
    "print(bi_tokens[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Create a list of bigrams of `review_0`. Save your results to `ans2` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "\n",
    "tokens = word_tokenize(review_0)\n",
    "ans2 = list(bigrams(tokens))\n",
    "\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('By', 'ERROL'),\n",
       " ('ERROL', 'MORRISNOV'),\n",
       " ('MORRISNOV', '.'),\n",
       " ('.', '10'),\n",
       " ('10', ','),\n",
       " (',', '2011'),\n",
       " ('2011', 'In'),\n",
       " ('In', 'all'),\n",
       " ('all', 'of'),\n",
       " ('of', 'Stephen'),\n",
       " ('Stephen', 'King'),\n",
       " ('King', '’'),\n",
       " ('’', 's'),\n",
       " ('s', 'work'),\n",
       " ('work', 'there'),\n",
       " ('there', 'is'),\n",
       " ('is', 'an'),\n",
       " ('an', 'admixture'),\n",
       " ('admixture', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'ordinary'),\n",
       " ('ordinary', 'and'),\n",
       " ('and', 'the'),\n",
       " ('the', 'supernatural'),\n",
       " ('supernatural', '—'),\n",
       " ('—', 'call'),\n",
       " ('call', 'it'),\n",
       " ('it', 'the'),\n",
       " ('the', 'weird'),\n",
       " ('weird', 'quotidian'),\n",
       " ('quotidian', '.'),\n",
       " ('.', 'In'),\n",
       " ('In', 'his'),\n",
       " ('his', 'new'),\n",
       " ('new', 'novel'),\n",
       " ('novel', ','),\n",
       " (',', '“'),\n",
       " ('“', '11/22/63'),\n",
       " ('11/22/63', ','),\n",
       " (',', '”'),\n",
       " ('”', 'it'),\n",
       " ('it', 'is'),\n",
       " ('is', 'a'),\n",
       " ('a', 'rabbit'),\n",
       " ('rabbit', 'hole'),\n",
       " ('hole', 'into'),\n",
       " ('into', 'the'),\n",
       " ('the', 'past'),\n",
       " ('past', 'that'),\n",
       " ('that', 'pops'),\n",
       " ('pops', 'up'),\n",
       " ('up', 'in'),\n",
       " ('in', 'Lisbon'),\n",
       " ('Lisbon', 'Falls'),\n",
       " ('Falls', ','),\n",
       " (',', 'a'),\n",
       " ('a', 'woebegone'),\n",
       " ('woebegone', 'corner'),\n",
       " ('corner', 'of'),\n",
       " ('of', 'Maine'),\n",
       " ('Maine', '.'),\n",
       " ('.', 'On'),\n",
       " ('On', 'one'),\n",
       " ('one', 'end'),\n",
       " ('end', 'is'),\n",
       " ('is', '2011'),\n",
       " ('2011', '.'),\n",
       " ('.', 'An'),\n",
       " ('An', 'unpopular'),\n",
       " ('unpopular', 'diner'),\n",
       " ('diner', 'has'),\n",
       " ('has', 'finally'),\n",
       " ('finally', 'been'),\n",
       " ('been', 'bought'),\n",
       " ('bought', 'out'),\n",
       " ('out', 'by'),\n",
       " ('by', 'L.'),\n",
       " ('L.', 'L.'),\n",
       " ('L.', 'Bean'),\n",
       " ('Bean', '.'),\n",
       " ('.', 'The'),\n",
       " ('The', 'diner'),\n",
       " ('diner', '—'),\n",
       " ('—', 'and'),\n",
       " ('and', 'the'),\n",
       " ('the', 'time'),\n",
       " ('time', 'portal'),\n",
       " ('portal', 'inside'),\n",
       " ('inside', 'it'),\n",
       " ('it', '—'),\n",
       " ('—', 'may'),\n",
       " ('may', 'last'),\n",
       " ('last', 'a'),\n",
       " ('a', 'few'),\n",
       " ('few', 'more'),\n",
       " ('more', 'weeks'),\n",
       " ('weeks', 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'footprint'),\n",
       " ('footprint', 'of'),\n",
       " ('of', 'a'),\n",
       " ('a', 'burned'),\n",
       " ('burned', 'textile'),\n",
       " ('textile', 'mill'),\n",
       " ('mill', '.'),\n",
       " ('.', 'On'),\n",
       " ('On', 'the'),\n",
       " ('the', 'other'),\n",
       " ('other', 'end'),\n",
       " ('end', 'is'),\n",
       " ('is', 'America'),\n",
       " ('America', 'under'),\n",
       " ('under', 'Eisenhower'),\n",
       " ('Eisenhower', '.'),\n",
       " ('.', 'The'),\n",
       " ('The', 'mill'),\n",
       " ('mill', 'churns'),\n",
       " ('churns', 'out'),\n",
       " ('out', 'white'),\n",
       " ('white', 'smoke'),\n",
       " ('smoke', '.'),\n",
       " ('.', '“'),\n",
       " ('“', 'Vertigo'),\n",
       " ('Vertigo', '”'),\n",
       " ('”', 'is'),\n",
       " ('is', 'showing'),\n",
       " ('showing', 'at'),\n",
       " ('at', 'the'),\n",
       " ('the', 'outdoor'),\n",
       " ('outdoor', 'movie'),\n",
       " ('movie', 'theater'),\n",
       " ('theater', '—'),\n",
       " ('—', 'on'),\n",
       " ('on', 'its'),\n",
       " ('its', 'first'),\n",
       " ('first', 'run'),\n",
       " ('run', '.'),\n",
       " ('.', 'The'),\n",
       " ('The', 'Kennebec'),\n",
       " ('Kennebec', 'Fruit'),\n",
       " ('Fruit', 'Company'),\n",
       " ('Company', 'isn'),\n",
       " ('isn', '’'),\n",
       " ('’', 't'),\n",
       " ('t', 'a'),\n",
       " ('a', 'curio'),\n",
       " ('curio', 'for'),\n",
       " ('for', 'tourists'),\n",
       " ('tourists', ';'),\n",
       " (';', 'it'),\n",
       " ('it', 'sells'),\n",
       " ('sells', 'oranges'),\n",
       " ('oranges', '.'),\n",
       " ('.', 'And'),\n",
       " ('And', 'John'),\n",
       " ('John', 'Kennedy'),\n",
       " ('Kennedy', ','),\n",
       " (',', 'the'),\n",
       " ('the', 'young'),\n",
       " ('young', 'senator'),\n",
       " ('senator', 'from'),\n",
       " ('from', 'Massachusetts'),\n",
       " ('Massachusetts', ','),\n",
       " (',', 'is'),\n",
       " ('is', 'still'),\n",
       " ('still', 'alive'),\n",
       " ('alive', '.'),\n",
       " ('.', 'The'),\n",
       " ('The', 'rules'),\n",
       " ('rules', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'rabbit'),\n",
       " ('rabbit', 'hole'),\n",
       " ('hole', 'into'),\n",
       " ('into', 'the'),\n",
       " ('the', 'past'),\n",
       " ('past', 'are'),\n",
       " ('are', 'outlined'),\n",
       " ('outlined', 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'first'),\n",
       " ('first', 'pages'),\n",
       " ('pages', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'novel'),\n",
       " ('novel', '.'),\n",
       " ('.', 'Al'),\n",
       " ('Al', 'Templeton'),\n",
       " ('Templeton', ','),\n",
       " (',', 'the'),\n",
       " ('the', 'owner'),\n",
       " ('owner', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'diner'),\n",
       " ('diner', ','),\n",
       " (',', 'explains'),\n",
       " ('explains', 'them'),\n",
       " ('them', 'to'),\n",
       " ('to', 'Jake'),\n",
       " ('Jake', 'Epping'),\n",
       " ('Epping', ','),\n",
       " (',', 'an'),\n",
       " ('an', 'English'),\n",
       " ('English', 'teacher'),\n",
       " ('teacher', 'at'),\n",
       " ('at', 'the'),\n",
       " ('the', 'local'),\n",
       " ('local', 'high'),\n",
       " ('high', 'school'),\n",
       " ('school', '.'),\n",
       " ('.', 'Walk'),\n",
       " ('Walk', 'to'),\n",
       " ('to', 'the'),\n",
       " ('the', 'back'),\n",
       " ('back', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'pantry'),\n",
       " ('pantry', '.'),\n",
       " ('.', 'Mind'),\n",
       " ('Mind', 'the'),\n",
       " ('the', '60-watt'),\n",
       " ('60-watt', 'bulb'),\n",
       " ('bulb', 'overhead'),\n",
       " ('overhead', '.'),\n",
       " ('.', 'Expect'),\n",
       " ('Expect', 'the'),\n",
       " ('the', 'smell'),\n",
       " ('smell', 'of'),\n",
       " ('of', 'sulfur'),\n",
       " ('sulfur', '.'),\n",
       " ('.', 'And'),\n",
       " ('And', 'keep'),\n",
       " ('keep', 'walking'),\n",
       " ('walking', 'until'),\n",
       " ('until', 'you'),\n",
       " ('you', 'feel'),\n",
       " ('feel', 'your'),\n",
       " ('your', 'foot'),\n",
       " ('foot', 'fall'),\n",
       " ('fall', '.'),\n",
       " ('.', 'Suddenly'),\n",
       " ('Suddenly', 'you'),\n",
       " ('you', '’'),\n",
       " ('’', 're'),\n",
       " ('re', 'back'),\n",
       " ('back', 'on'),\n",
       " ('on', 'Sept.'),\n",
       " ('Sept.', '9'),\n",
       " ('9', ','),\n",
       " (',', '1958'),\n",
       " ('1958', '.'),\n",
       " ('.', 'It'),\n",
       " ('It', '’'),\n",
       " ('’', 's'),\n",
       " ('s', '11:58'),\n",
       " ('11:58', 'a.m'),\n",
       " ('a.m', '.'),\n",
       " ('.', 'There'),\n",
       " ('There', 'are'),\n",
       " ('are', ','),\n",
       " (',', 'Al'),\n",
       " ('Al', 'says'),\n",
       " ('says', ','),\n",
       " (',', 'only'),\n",
       " ('only', 'two'),\n",
       " ('two', 'conditions'),\n",
       " ('conditions', '.'),\n",
       " ('.', 'One'),\n",
       " ('One', ','),\n",
       " (',', 'it'),\n",
       " ('it', '’'),\n",
       " ('’', 's'),\n",
       " ('s', 'not'),\n",
       " ('not', 'a'),\n",
       " ('a', 'one-way'),\n",
       " ('one-way', 'trip'),\n",
       " ('trip', '.'),\n",
       " ('.', 'It'),\n",
       " ('It', 'doesn'),\n",
       " ('doesn', '’'),\n",
       " ('’', 't'),\n",
       " ('t', 'have'),\n",
       " ('have', 'to'),\n",
       " ('to', 'be'),\n",
       " ('be', '.'),\n",
       " ('.', 'But'),\n",
       " ('But', 'when'),\n",
       " ('when', 'you'),\n",
       " ('you', 'return'),\n",
       " ('return', ','),\n",
       " (',', 'no'),\n",
       " ('no', 'matter'),\n",
       " ('matter', 'how'),\n",
       " ('how', 'long'),\n",
       " ('long', 'you'),\n",
       " ('you', '’'),\n",
       " ('’', 've'),\n",
       " ('ve', 'stayed'),\n",
       " ('stayed', 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'past'),\n",
       " ('past', '—'),\n",
       " ('—', 'two'),\n",
       " ('two', 'days'),\n",
       " ('days', ','),\n",
       " (',', 'five'),\n",
       " ('five', 'years'),\n",
       " ('years', ','),\n",
       " (',', 'whatever'),\n",
       " ('whatever', '—'),\n",
       " ('—', 'only'),\n",
       " ('only', 'two'),\n",
       " ('two', 'minutes'),\n",
       " ('minutes', 'have'),\n",
       " ('have', 'gone'),\n",
       " ('gone', 'by'),\n",
       " ('by', 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'present'),\n",
       " ('present', '.'),\n",
       " ('.', 'Two'),\n",
       " ('Two', ','),\n",
       " (',', 'each'),\n",
       " ('each', 'time'),\n",
       " ('time', 'you'),\n",
       " ('you', 'go'),\n",
       " ('go', 'back'),\n",
       " ('back', 'to'),\n",
       " ('to', 'the'),\n",
       " ('the', 'past'),\n",
       " ('past', ','),\n",
       " (',', 'there'),\n",
       " ('there', 'is'),\n",
       " ('is', 'a'),\n",
       " ('a', 'reset'),\n",
       " ('reset', '.'),\n",
       " ('.', 'Like'),\n",
       " ('Like', 'a'),\n",
       " ('a', 'Magic'),\n",
       " ('Magic', 'Slate'),\n",
       " ('Slate', '.'),\n",
       " ('.', 'It'),\n",
       " ('It', '’'),\n",
       " ('’', 's'),\n",
       " ('s', '11:58'),\n",
       " ('11:58', 'a.m.'),\n",
       " ('a.m.', ','),\n",
       " (',', 'and'),\n",
       " ('and', 'everything'),\n",
       " ('everything', 'you'),\n",
       " ('you', 'did'),\n",
       " ('did', 'on'),\n",
       " ('on', 'your'),\n",
       " ('your', 'previous'),\n",
       " ('previous', 'trip'),\n",
       " ('trip', 'has'),\n",
       " ('has', 'been'),\n",
       " ('been', 'erased'),\n",
       " ('erased', '.'),\n",
       " ('.', 'With'),\n",
       " ('With', 'that'),\n",
       " ('that', ','),\n",
       " (',', 'King'),\n",
       " ('King', 'dispenses'),\n",
       " ('dispenses', 'with'),\n",
       " ('with', 'many'),\n",
       " ('many', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'mechanics'),\n",
       " ('mechanics', 'of'),\n",
       " ('of', 'time-travel'),\n",
       " ('time-travel', '—'),\n",
       " ('—', 'and'),\n",
       " ('and', 'thank'),\n",
       " ('thank', 'God'),\n",
       " ('God', 'for'),\n",
       " ('for', 'it'),\n",
       " ('it', '.'),\n",
       " ('.', 'There'),\n",
       " ('There', 'is'),\n",
       " ('is', 'no'),\n",
       " ('no', 'extended'),\n",
       " ('extended', 'discussion'),\n",
       " ('discussion', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', '“'),\n",
       " ('“', 'grandfather'),\n",
       " ('grandfather', 'paradox.'),\n",
       " ('paradox.', '”'),\n",
       " ('”', '('),\n",
       " ('(', '“'),\n",
       " ('“', 'What'),\n",
       " ('What', 'if'),\n",
       " ('if', 'you'),\n",
       " ('you', 'killed'),\n",
       " ('killed', 'your'),\n",
       " ('your', 'grandfather'),\n",
       " ('grandfather', '?'),\n",
       " ('?', '”'),\n",
       " ('”', '“'),\n",
       " ('“', 'Why'),\n",
       " ('Why', 'on'),\n",
       " ('on', 'earth'),\n",
       " ('earth', 'would'),\n",
       " ('would', 'you'),\n",
       " ('you', 'do'),\n",
       " ('do', 'that'),\n",
       " ('that', '?'),\n",
       " ('?', '”'),\n",
       " ('”', ')'),\n",
       " (')', 'The'),\n",
       " ('The', 'rules'),\n",
       " ('rules', 'are'),\n",
       " ('are', 'simple'),\n",
       " ('simple', '.'),\n",
       " ('.', 'There'),\n",
       " ('There', 'is'),\n",
       " ('is', 'a'),\n",
       " ('a', 'reason'),\n",
       " ('reason', 'for'),\n",
       " ('for', 'this'),\n",
       " ('this', ':'),\n",
       " (':', 'King'),\n",
       " ('King', 'is'),\n",
       " ('is', 'after'),\n",
       " ('after', 'something'),\n",
       " ('something', 'bigger'),\n",
       " ('bigger', '.'),\n",
       " ('.', '“'),\n",
       " ('“', '11/22/63'),\n",
       " ('11/22/63', '”'),\n",
       " ('”', 'is'),\n",
       " ('is', 'a'),\n",
       " ('a', 'meditation'),\n",
       " ('meditation', 'on'),\n",
       " ('on', 'memory'),\n",
       " ('memory', ','),\n",
       " (',', 'love'),\n",
       " ('love', ','),\n",
       " (',', 'loss'),\n",
       " ('loss', ','),\n",
       " (',', 'free'),\n",
       " ('free', 'will'),\n",
       " ('will', 'and'),\n",
       " ('and', 'necessity'),\n",
       " ('necessity', '.'),\n",
       " ('.', 'It'),\n",
       " ('It', '’'),\n",
       " ('’', 's'),\n",
       " ('s', 'a'),\n",
       " ('a', 'blunderbuss'),\n",
       " ('blunderbuss', 'of'),\n",
       " ('of', 'a'),\n",
       " ('a', 'book'),\n",
       " ('book', ','),\n",
       " (',', 'rife'),\n",
       " ('rife', 'with'),\n",
       " ('with', 'answers'),\n",
       " ('answers', 'to'),\n",
       " ('to', 'questions'),\n",
       " ('questions', ':'),\n",
       " (':', 'Can'),\n",
       " ('Can', 'one'),\n",
       " ('one', 'man'),\n",
       " ('man', 'make'),\n",
       " ('make', 'a'),\n",
       " ('a', 'difference'),\n",
       " ('difference', '?'),\n",
       " ('?', 'Can'),\n",
       " ('Can', 'history'),\n",
       " ('history', 'be'),\n",
       " ('be', 'changed'),\n",
       " ('changed', ','),\n",
       " (',', 'or'),\n",
       " ('or', 'does'),\n",
       " ('does', 'it'),\n",
       " ('it', 'snap'),\n",
       " ('snap', 'back'),\n",
       " ('back', 'on'),\n",
       " ('on', 'itself'),\n",
       " ('itself', 'like'),\n",
       " ('like', 'a'),\n",
       " ('a', 'rubber'),\n",
       " ('rubber', 'band'),\n",
       " ('band', '?'),\n",
       " ('?', 'Does'),\n",
       " ('Does', 'love'),\n",
       " ('love', 'conquer'),\n",
       " ('conquer', 'all'),\n",
       " ('all', '?'),\n",
       " ('?', '('),\n",
       " ('(', 'The'),\n",
       " ('The', 'big'),\n",
       " ('big', 'stuff'),\n",
       " ('stuff', '.'),\n",
       " ('.', ')'),\n",
       " (')', 'Al'),\n",
       " ('Al', '—'),\n",
       " ('—', 'the'),\n",
       " ('the', 'scuttlebutt'),\n",
       " ('scuttlebutt', 'is'),\n",
       " ('is', 'that'),\n",
       " ('that', 'he'),\n",
       " ('he', 'is'),\n",
       " ('is', 'serving'),\n",
       " ('serving', 'burgers'),\n",
       " ('burgers', 'made'),\n",
       " ('made', 'of'),\n",
       " ('of', 'dog'),\n",
       " ('dog', ','),\n",
       " (',', 'or'),\n",
       " ('or', 'cat'),\n",
       " ('cat', '—'),\n",
       " ('—', 'is'),\n",
       " ('is', 'dying'),\n",
       " ('dying', 'of'),\n",
       " ('of', 'lung'),\n",
       " ('lung', 'cancer'),\n",
       " ('cancer', '.'),\n",
       " ('.', 'Coughing'),\n",
       " ('Coughing', 'up'),\n",
       " ('up', 'blood'),\n",
       " ('blood', 'into'),\n",
       " ('into', 'a'),\n",
       " ('a', 'pile'),\n",
       " ('pile', 'of'),\n",
       " ('of', 'maxi-pads'),\n",
       " ('maxi-pads', '.'),\n",
       " ('.', 'He'),\n",
       " ('He', 'enlists'),\n",
       " ('enlists', 'Jake'),\n",
       " ('Jake', 'to'),\n",
       " ('to', 'do'),\n",
       " ('do', 'what'),\n",
       " ('what', 'he'),\n",
       " ('he', 'couldn'),\n",
       " ('couldn', '’'),\n",
       " ('’', 't'),\n",
       " ('t', ':'),\n",
       " (':', 'stop'),\n",
       " ('stop', 'Lee'),\n",
       " ('Lee', 'Harvey'),\n",
       " ('Harvey', 'Oswald'),\n",
       " ('Oswald', '.'),\n",
       " ('.', 'It'),\n",
       " ('It', '’'),\n",
       " ('’', 's'),\n",
       " ('s', 'a'),\n",
       " ('a', 'fabulous'),\n",
       " ('fabulous', 'pitch'),\n",
       " ('pitch', '.'),\n",
       " ('.', '“'),\n",
       " ('“', 'Save'),\n",
       " ('Save', 'Kennedy'),\n",
       " ('Kennedy', ','),\n",
       " (',', 'save'),\n",
       " ('save', 'his'),\n",
       " ('his', 'brother'),\n",
       " ('brother', '.'),\n",
       " ('.', 'Save'),\n",
       " ('Save', 'Martin'),\n",
       " ('Martin', 'Luther'),\n",
       " ('Luther', 'King'),\n",
       " ('King', '.'),\n",
       " ('.', 'Stop'),\n",
       " ('Stop', 'the'),\n",
       " ('the', 'race'),\n",
       " ('race', 'riots'),\n",
       " ('riots', '.'),\n",
       " ('.', 'Stop'),\n",
       " ('Stop', 'Vietnam'),\n",
       " ('Vietnam', ','),\n",
       " (',', 'maybe'),\n",
       " ('maybe', '.'),\n",
       " ('.', '.'),\n",
       " ('.', '.'),\n",
       " ('.', '.'),\n",
       " ('.', 'Get'),\n",
       " ('Get', 'rid'),\n",
       " ('rid', 'of'),\n",
       " ('of', 'one'),\n",
       " ('one', 'wretched'),\n",
       " ('wretched', 'waif'),\n",
       " ('waif', ','),\n",
       " (',', 'buddy'),\n",
       " ('buddy', ','),\n",
       " (',', 'and'),\n",
       " ('and', 'you'),\n",
       " ('you', 'could'),\n",
       " ('could', 'save'),\n",
       " ('save', 'millions'),\n",
       " ('millions', 'of'),\n",
       " ('of', 'lives.'),\n",
       " ('lives.', '”'),\n",
       " ('”', 'Jake'),\n",
       " ('Jake', 'Epping'),\n",
       " ('Epping', 'is'),\n",
       " ('is', 'a'),\n",
       " ('a', 'burned-out'),\n",
       " ('burned-out', 'teacher'),\n",
       " ('teacher', 'with'),\n",
       " ('with', 'a'),\n",
       " ('a', 'seriously'),\n",
       " ('seriously', 'alcoholic'),\n",
       " ('alcoholic', 'ex-wife'),\n",
       " ('ex-wife', 'and'),\n",
       " ('and', 'nothing'),\n",
       " ('nothing', 'better'),\n",
       " ('better', 'to'),\n",
       " ('to', 'do'),\n",
       " ('do', 'than'),\n",
       " ('than', 'disappear'),\n",
       " ('disappear', 'into'),\n",
       " ('into', 'the'),\n",
       " ('the', 'past'),\n",
       " ('past', '.'),\n",
       " ('.', 'The'),\n",
       " ('The', 'guilt'),\n",
       " ('guilt', 'trip'),\n",
       " ('trip', 'works'),\n",
       " ('works', '.'),\n",
       " ('.', 'And'),\n",
       " ('And', 'Epping'),\n",
       " ('Epping', 'falls'),\n",
       " ('falls', 'into'),\n",
       " ('into', 'the'),\n",
       " ('the', 'past'),\n",
       " ('past', 'with'),\n",
       " ('with', 'a'),\n",
       " ('a', 'new'),\n",
       " ('new', 'name'),\n",
       " ('name', ','),\n",
       " (',', 'George'),\n",
       " ('George', 'T.'),\n",
       " ('T.', 'Amberson'),\n",
       " ('Amberson', '—'),\n",
       " ('—', 'as'),\n",
       " ('as', 'if'),\n",
       " ('if', 'time-travel'),\n",
       " ('time-travel', 'required'),\n",
       " ('required', 'a'),\n",
       " ('a', 'new'),\n",
       " ('new', 'identity'),\n",
       " ('identity', '—'),\n",
       " ('—', 'and'),\n",
       " ('and', 'a'),\n",
       " ('a', 'clear'),\n",
       " ('clear', 'mission'),\n",
       " ('mission', '.'),\n",
       " ('.', 'Correct'),\n",
       " ('Correct', 'the'),\n",
       " ('the', 'past'),\n",
       " ('past', '.'),\n",
       " ('.', 'Undo'),\n",
       " ('Undo', 'some'),\n",
       " ('some', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'evils'),\n",
       " ('evils', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', '20th'),\n",
       " ('20th', 'century'),\n",
       " ('century', '.'),\n",
       " ('.', 'Once'),\n",
       " ('Once', 'in'),\n",
       " ('in', '1958'),\n",
       " ('1958', ','),\n",
       " (',', 'however'),\n",
       " ('however', ','),\n",
       " (',', 'Amberson'),\n",
       " ('Amberson', 'is'),\n",
       " ('is', 'immediately'),\n",
       " ('immediately', 'confronted'),\n",
       " ('confronted', 'by'),\n",
       " ('by', 'a'),\n",
       " ('a', 'double'),\n",
       " ('double', 'mystery'),\n",
       " ('mystery', ':'),\n",
       " (':', 'the'),\n",
       " ('the', 'mystery'),\n",
       " ('mystery', 'of'),\n",
       " ('of', 'what'),\n",
       " ('what', 'really'),\n",
       " ('really', 'happened'),\n",
       " ('happened', 'then'),\n",
       " ('then', ','),\n",
       " (',', 'and'),\n",
       " ('and', 'the'),\n",
       " ('the', 'mystery'),\n",
       " ('mystery', 'of'),\n",
       " ('of', 'what'),\n",
       " ('what', 'might'),\n",
       " ('might', 'be'),\n",
       " ('be', 'otherwise'),\n",
       " ('otherwise', '.'),\n",
       " ('.', 'Before'),\n",
       " ('Before', 'George/Jake'),\n",
       " ('George/Jake', 'can'),\n",
       " ('can', 'alter'),\n",
       " ('alter', 'the'),\n",
       " ('the', 'course'),\n",
       " ('course', 'of'),\n",
       " ('of', 'history'),\n",
       " ('history', ','),\n",
       " (',', 'he'),\n",
       " ('he', 'has'),\n",
       " ('has', 'to'),\n",
       " ('to', 'know'),\n",
       " ('know', 'what'),\n",
       " ('what', 'actually'),\n",
       " ('actually', 'occurred'),\n",
       " ('occurred', '.'),\n",
       " ('.', 'Was'),\n",
       " ('Was', 'it'),\n",
       " ('it', 'Oswald'),\n",
       " ('Oswald', ','),\n",
       " (',', 'shooting'),\n",
       " ('shooting', 'from'),\n",
       " ('from', 'the'),\n",
       " ('the', 'depository'),\n",
       " ('depository', '?'),\n",
       " ('?', 'Was'),\n",
       " ('Was', 'it'),\n",
       " ('it', 'a'),\n",
       " ('a', 'conspiracy'),\n",
       " ('conspiracy', '?'),\n",
       " ('?', 'Another'),\n",
       " ('Another', 'shooter'),\n",
       " ('shooter', 'on'),\n",
       " ('on', 'the'),\n",
       " ('the', 'grassy'),\n",
       " ('grassy', 'knoll'),\n",
       " ('knoll', '?'),\n",
       " ('?', 'How'),\n",
       " ('How', 'about'),\n",
       " ('about', 'George'),\n",
       " ('George', 'de'),\n",
       " ('de', 'Mohrenschildt'),\n",
       " ('Mohrenschildt', ','),\n",
       " (',', 'one'),\n",
       " ('one', 'persistent'),\n",
       " ('persistent', 'minor'),\n",
       " ('minor', 'character'),\n",
       " ('character', 'in'),\n",
       " ('in', 'conspiracy'),\n",
       " ('conspiracy', 'thinking'),\n",
       " ('thinking', '?'),\n",
       " ('?', 'They'),\n",
       " ('They', 'are'),\n",
       " ('are', 'the'),\n",
       " ('the', 'nightmare'),\n",
       " ('nightmare', 'uncertainties'),\n",
       " ('uncertainties', 'of'),\n",
       " ('of', 'an'),\n",
       " ('an', 'event'),\n",
       " ('event', 'that'),\n",
       " ('that', 'has'),\n",
       " ('has', 'been'),\n",
       " ('been', 'over-examined'),\n",
       " ('over-examined', ','),\n",
       " (',', 'and'),\n",
       " ('and', 'never'),\n",
       " ('never', 'understood'),\n",
       " ('understood', '.'),\n",
       " ('.', 'Jake'),\n",
       " ('Jake', 'is'),\n",
       " ('is', 'a'),\n",
       " ('a', 'good'),\n",
       " ('good', 'person'),\n",
       " ('person', '.'),\n",
       " ('.', 'He'),\n",
       " ('He', 'can'),\n",
       " ('can', 'not'),\n",
       " ('not', 'kill'),\n",
       " ('kill', 'Oswald'),\n",
       " ('Oswald', 'without'),\n",
       " ('without', 'first'),\n",
       " ('first', 'knowing'),\n",
       " ('knowing', 'whether'),\n",
       " ('whether', 'he'),\n",
       " ('he', 'was'),\n",
       " ('was', 'the'),\n",
       " ('the', 'responsible'),\n",
       " ('responsible', 'party'),\n",
       " ('party', ','),\n",
       " (',', 'and'),\n",
       " ('and', 'a'),\n",
       " ('a', 'good'),\n",
       " ('good', 'part'),\n",
       " ('part', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'adventure'),\n",
       " ('adventure', 'is'),\n",
       " ('is', 'the'),\n",
       " ('the', 'investigation'),\n",
       " ('investigation', '.'),\n",
       " ('.', 'Once'),\n",
       " ('Once', 'in'),\n",
       " ('in', 'Dallas'),\n",
       " ('Dallas', ','),\n",
       " (',', 'Amberson'),\n",
       " ('Amberson', 'has'),\n",
       " ('has', 'years'),\n",
       " ('years', 'to'),\n",
       " ('to', 'get'),\n",
       " ('get', 'to'),\n",
       " ('to', 'know'),\n",
       " ('know', 'Oswald'),\n",
       " ('Oswald', ','),\n",
       " (',', 'but'),\n",
       " ('but', 'he'),\n",
       " ('he', 'can'),\n",
       " ('can', '’'),\n",
       " ('’', 't'),\n",
       " ('t', 'just'),\n",
       " ('just', 'bust'),\n",
       " ('bust', 'down'),\n",
       " ('down', 'the'),\n",
       " ('the', 'door'),\n",
       " ('door', '.'),\n",
       " ('.', 'History'),\n",
       " ('History', 'is'),\n",
       " ('is', 'fragile'),\n",
       " ('fragile', ';'),\n",
       " (';', 'he'),\n",
       " ('he', 'has'),\n",
       " ('has', 'to'),\n",
       " ('to', 'peer'),\n",
       " ('peer', 'around'),\n",
       " ('around', 'corners'),\n",
       " ('corners', '.'),\n",
       " ('.', 'He'),\n",
       " ('He', 'buys'),\n",
       " ('buys', 'tape'),\n",
       " ('tape', 'recorders'),\n",
       " ('recorders', 'and'),\n",
       " ('and', 'long-distance'),\n",
       " ('long-distance', 'listening'),\n",
       " ('listening', 'devices'),\n",
       " ('devices', ','),\n",
       " (',', 'moves'),\n",
       " ('moves', 'into'),\n",
       " ('into', 'grubby'),\n",
       " ('grubby', 'neighborhoods'),\n",
       " ('neighborhoods', ','),\n",
       " (',', 'trails'),\n",
       " ('trails', 'Oswald'),\n",
       " ('Oswald', 'as'),\n",
       " ('as', 'he'),\n",
       " ('he', 'stashes'),\n",
       " ('stashes', 'his'),\n",
       " ('his', 'rifle'),\n",
       " ('rifle', '.'),\n",
       " ('.', 'What'),\n",
       " ('What', 'he'),\n",
       " ('he', 'learns'),\n",
       " ('learns', 'is'),\n",
       " ('is', 'no'),\n",
       " ('no', 'surprise'),\n",
       " ('surprise', '.'),\n",
       " ('.', 'Oswald'),\n",
       " ('Oswald', 'was'),\n",
       " ('was', 'unpleasant'),\n",
       " ('unpleasant', 'in'),\n",
       " ('in', 'ordinary'),\n",
       " ('ordinary', 'ways'),\n",
       " ('ways', '.'),\n",
       " ('.', 'Emotional'),\n",
       " ('Emotional', ','),\n",
       " (',', 'violent'),\n",
       " ('violent', 'with'),\n",
       " ('with', 'his'),\n",
       " ('his', 'wife'),\n",
       " ('wife', ','),\n",
       " (',', 'unsure'),\n",
       " ('unsure', 'of'),\n",
       " ('of', 'himself'),\n",
       " ('himself', 'and'),\n",
       " ('and', 'desperate'),\n",
       " ('desperate', 'to'),\n",
       " ('to', 'change'),\n",
       " ('change', 'a'),\n",
       " ('a', 'broken'),\n",
       " ('broken', 'world'),\n",
       " ('world', '.'),\n",
       " ('.', 'Did'),\n",
       " ('Did', 'he'),\n",
       " ('he', 'kill'),\n",
       " ('kill', 'Kennedy'),\n",
       " ('Kennedy', '?'),\n",
       " ('?', 'It'),\n",
       " ('It', '’'),\n",
       " ('’', 's'),\n",
       " ('s', 'easy'),\n",
       " ('easy', 'to'),\n",
       " ('to', 'see'),\n",
       " ('see', 'King'),\n",
       " ('King', ','),\n",
       " (',', 'the'),\n",
       " ('the', 'writer'),\n",
       " ('writer', 'and'),\n",
       " ('and', 'researcher'),\n",
       " ('researcher', ','),\n",
       " (',', 'as'),\n",
       " ('as', 'a'),\n",
       " ('a', 'fellow'),\n",
       " ('fellow', 'time-traveler'),\n",
       " ('time-traveler', ','),\n",
       " (',', 'hopelessly'),\n",
       " ('hopelessly', 'curious'),\n",
       " ('curious', 'about'),\n",
       " ('about', 'what'),\n",
       " ('what', 'Oswald'),\n",
       " ('Oswald', 'might'),\n",
       " ('might', 'say'),\n",
       " ('say', 'on'),\n",
       " ('on', 'tape'),\n",
       " ('tape', 'or'),\n",
       " ('or', 'reveal'),\n",
       " ('reveal', 'while'),\n",
       " ('while', 'strolling'),\n",
       " ('strolling', 'around'),\n",
       " ('around', 'Fort'),\n",
       " ('Fort', 'Worth'),\n",
       " ('Worth', '.'),\n",
       " ('.', 'But'),\n",
       " ('But', 'the'),\n",
       " ('the', 'past'),\n",
       " ('past', ','),\n",
       " (',', 'the'),\n",
       " ('the', 'novel'),\n",
       " ('novel', 'repeatedly'),\n",
       " ('repeatedly', 'reminds'),\n",
       " ('reminds', 'us'),\n",
       " ('us', ','),\n",
       " (',', 'is'),\n",
       " ('is', 'obdurate'),\n",
       " ('obdurate', '.'),\n",
       " ('.', 'Under'),\n",
       " ('Under', 'interrogation'),\n",
       " ('interrogation', ','),\n",
       " (',', 'it'),\n",
       " ('it', 'guards'),\n",
       " ('guards', 'its'),\n",
       " ('its', 'darkest'),\n",
       " ('darkest', 'secrets'),\n",
       " ('secrets', '.'),\n",
       " ('.', 'Weeks'),\n",
       " ('Weeks', 'before'),\n",
       " ('before', 'the'),\n",
       " ('the', '22nd'),\n",
       " ('22nd', ','),\n",
       " (',', 'Amberson'),\n",
       " ('Amberson', 'is'),\n",
       " ('is', 'living'),\n",
       " ('living', 'below'),\n",
       " ('below', 'the'),\n",
       " ('the', 'Oswalds'),\n",
       " ('Oswalds', ','),\n",
       " (',', 'and'),\n",
       " ('and', 'he'),\n",
       " ('he', 'still'),\n",
       " ('still', 'can'),\n",
       " ('can', '’'),\n",
       " ('’', 't'),\n",
       " ('t', 'be'),\n",
       " ('be', 'sure'),\n",
       " ('sure', ':'),\n",
       " (':', '“'),\n",
       " ('“', 'I'),\n",
       " ('I', 'tried'),\n",
       " ('tried', 'the'),\n",
       " ('the', 'distance'),\n",
       " ('distance', 'mic'),\n",
       " ('mic', ','),\n",
       " (',', 'standing'),\n",
       " ('standing', 'on'),\n",
       " ('on', 'a'),\n",
       " ('a', 'chair'),\n",
       " ('chair', 'and'),\n",
       " ('and', 'holding'),\n",
       " ('holding', 'the'),\n",
       " ('the', 'Tupperware'),\n",
       " ('Tupperware', 'bowl'),\n",
       " ('bowl', 'almost'),\n",
       " ('almost', 'against'),\n",
       " ('against', 'the'),\n",
       " ('the', 'ceiling'),\n",
       " ('ceiling', '.'),\n",
       " ('.', 'With'),\n",
       " ('With', 'it'),\n",
       " ('it', 'I'),\n",
       " ('I', 'could'),\n",
       " ('could', 'hear'),\n",
       " ('hear', 'Lee'),\n",
       " ('Lee', 'talking'),\n",
       " ('talking', 'and'),\n",
       " ('and', 'de'),\n",
       " ('de', 'Mohrenschildt'),\n",
       " ('Mohrenschildt', '’'),\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 02",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q3'></a>\n",
    "\n",
    "### Question 3:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "\n",
    "\n",
    "Once we have a list of tokens or bigrams, we can use the `FreqDist` class to create a count of the tokens.  From here, we can view the most common words and their counts using the `.most_common()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('she', 8), ('was', 6), ('and', 5), ('.', 5), ('to', 4)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''\n",
    "As she said this, she looked down at her hands and was surprised to see\n",
    "that she had put on one of the Rabbit's little white kid-gloves while\n",
    "she was talking. \"How _can_ I have done that?\" she thought. \"I must be\n",
    "growing small again.\" She got up and went to the table to measure\n",
    "herself by it and found that she was now about two feet high and was\n",
    "going on shrinking rapidly. She soon found out that the cause of this\n",
    "was the fan she was holding and she dropped it hastily, just in time to\n",
    "save herself from shrinking away altogether.\n",
    "'''\n",
    "tokens = word_tokenize(text)\n",
    "fdist = FreqDist(tokens)\n",
    "fdist.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Complete the function `top_n` below that will accept a single string of text and an integer, `n`. Your function should return a list of tuples of the `n` most common tokens from the text of the form (token, count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def top_n(text, n):\n",
    "    tokens = word_tokenize(text)\n",
    "    fdist = FreqDist(tokens)\n",
    "    return fdist.most_common(n)\n",
    "\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('she', 8), ('was', 6), ('and', 5), ('.', 5), ('to', 4)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n(text,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 03",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q4'></a>\n",
    "\n",
    "### Question 4:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "`scikitlearn` has similar functionality to that of the NLTK library for text tokenization.  It is especially well suited to working with dataframes so we will bring in a dataset labeled with sentiment from the IMDB Movie Reviews dataset curated by Maas.  This is a small sample of the data, if you are interested in the entire dataset feel free to visit: http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "imdb = pd.read_csv('./data/sample_revs.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Badland is one of the worst movies I ever seen...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New Year's Day. The day after consuming a few ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There's really not a whole lot to say about th...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>...for this movie defines a new low in Bollywo...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What seemed at first just another introverted ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  Badland is one of the worst movies I ever seen...  negative\n",
       "1  New Year's Day. The day after consuming a few ...  negative\n",
       "2  There's really not a whole lot to say about th...  negative\n",
       "3  ...for this movie defines a new low in Bollywo...  negative\n",
       "4  What seemed at first just another introverted ...  positive"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "As you can see, we have the text of movie reviews that have been given labels of either \"positive\" or \"negative\".  We aim to use train a logistic regression classifier using the tokenized version of the reviews as input, and the sentiment feature as the target.  To begin, we can create our input array using the `CountVectorizer` class.\n",
    "\n",
    "The `CountVectorizer` is a transformer, for which we will use the `.fit_transform()` method to create a [sparse array](https://en.wikipedia.org/wiki/Sparse_matrix).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#instantiate the class\n",
    "cvect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#create a document term matrix from king reviews\n",
    "dtm = cvect.fit_transform(df.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00', '000', '02', '03844', '074', '10', '100', '103', '11', '12']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#examine the first ten tokens\n",
    "#similar to word_tokenize from earlier\n",
    "cvect.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>02</th>\n",
       "      <th>03844</th>\n",
       "      <th>074</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>103</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>...</th>\n",
       "      <th>zeitgeist</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zeros</th>\n",
       "      <th>zestful</th>\n",
       "      <th>zevon</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zombified</th>\n",
       "      <th>zone</th>\n",
       "      <th>zuckerman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9724 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  02  03844  074  10  100  103  11  12  ...  zeitgeist  zeppelin  \\\n",
       "0   0    0   0      0    0   1    0    0   7   0  ...          0         0   \n",
       "1   0    0   0      0    0   0    0    0   6   0  ...          0         0   \n",
       "2   0    0   0      0    0   0    0    0   0   0  ...          0         0   \n",
       "3   0    0   0      0    0   0    0    0   0   0  ...          0         0   \n",
       "4   0    0   0      0    0   0    0    0   0   0  ...          0         0   \n",
       "\n",
       "   zeros  zestful  zevon  zombie  zombies  zombified  zone  zuckerman  \n",
       "0      0        0      0       0        0          0     0          0  \n",
       "1      0        0      0       0        0          0     0          0  \n",
       "2      0        0      0       0        0          0     0          0  \n",
       "3      0        1      0       0        0          0     0          0  \n",
       "4      0        0      0       0        0          0     0          0  \n",
       "\n",
       "[5 rows x 9724 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert sparse array to dense array\n",
    "dtm = dtm.todense()\n",
    "#get column names\n",
    "features = cvect.get_feature_names()\n",
    "#build DataFrame\n",
    "dtm_df = pd.DataFrame(dtm,  columns = features)\n",
    "dtm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>02</th>\n",
       "      <th>03844</th>\n",
       "      <th>074</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>103</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>...</th>\n",
       "      <th>zeitgeist</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zeros</th>\n",
       "      <th>zestful</th>\n",
       "      <th>zevon</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zombified</th>\n",
       "      <th>zone</th>\n",
       "      <th>zuckerman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 9724 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000  02  03844  074  10  100  103  11  12  ...  zeitgeist  zeppelin  \\\n",
       "0    0    0   0      0    0   1    0    0   7   0  ...          0         0   \n",
       "1    0    0   0      0    0   0    0    0   6   0  ...          0         0   \n",
       "2    0    0   0      0    0   0    0    0   0   0  ...          0         0   \n",
       "3    0    0   0      0    0   0    0    0   0   0  ...          0         0   \n",
       "4    0    0   0      0    0   0    0    0   0   0  ...          0         0   \n",
       "..  ..  ...  ..    ...  ...  ..  ...  ...  ..  ..  ...        ...       ...   \n",
       "59   0    0   0      0    0   0    1    0   0   0  ...          0         0   \n",
       "60   0    1   0      0    0   0    0    0   0   0  ...          1         0   \n",
       "61   0    0   0      0    0   0    0    0   0   0  ...          0         0   \n",
       "62   0    0   0      0    0   0    0    0   0   2  ...          0         0   \n",
       "63   0    0   0      0    0   1    0    0   0   0  ...          0         0   \n",
       "\n",
       "    zeros  zestful  zevon  zombie  zombies  zombified  zone  zuckerman  \n",
       "0       0        0      0       0        0          0     0          0  \n",
       "1       0        0      0       0        0          0     0          0  \n",
       "2       0        0      0       0        0          0     0          0  \n",
       "3       0        1      0       0        0          0     0          0  \n",
       "4       0        0      0       0        0          0     0          0  \n",
       "..    ...      ...    ...     ...      ...        ...   ...        ...  \n",
       "59      0        0      0       2        2          0     0          0  \n",
       "60      0        0      0       0        0          0     0          3  \n",
       "61      0        0      0       0        0          0     0          0  \n",
       "62      0        0      0       0        0          0     0          0  \n",
       "63      0        0      0       0        0          0     1          0  \n",
       "\n",
       "[64 rows x 9724 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Write a function, `dtm_maker` that accepts a series of string objects and returns a DataFrame of tokens and counts for each row in the series. The term will be a column header, and the count value will be a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def dtm_maker(series):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    cvect = CountVectorizer()\n",
    "    dtm = cvect.fit_transform(series)\n",
    "    dtm = dtm.todense()\n",
    "    features = cvect.get_feature_names()\n",
    "    return  pd.DataFrame(dtm,  columns = features)\n",
    "\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>02</th>\n",
       "      <th>03844</th>\n",
       "      <th>074</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>103</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>...</th>\n",
       "      <th>zeitgeist</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zeros</th>\n",
       "      <th>zestful</th>\n",
       "      <th>zevon</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zombified</th>\n",
       "      <th>zone</th>\n",
       "      <th>zuckerman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 9724 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000  02  03844  074  10  100  103  11  12  ...  zeitgeist  zeppelin  \\\n",
       "0    0    0   0      0    0   1    0    0   7   0  ...          0         0   \n",
       "1    0    0   0      0    0   0    0    0   6   0  ...          0         0   \n",
       "2    0    0   0      0    0   0    0    0   0   0  ...          0         0   \n",
       "3    0    0   0      0    0   0    0    0   0   0  ...          0         0   \n",
       "4    0    0   0      0    0   0    0    0   0   0  ...          0         0   \n",
       "..  ..  ...  ..    ...  ...  ..  ...  ...  ..  ..  ...        ...       ...   \n",
       "59   0    0   0      0    0   0    1    0   0   0  ...          0         0   \n",
       "60   0    1   0      0    0   0    0    0   0   0  ...          1         0   \n",
       "61   0    0   0      0    0   0    0    0   0   0  ...          0         0   \n",
       "62   0    0   0      0    0   0    0    0   0   2  ...          0         0   \n",
       "63   0    0   0      0    0   1    0    0   0   0  ...          0         0   \n",
       "\n",
       "    zeros  zestful  zevon  zombie  zombies  zombified  zone  zuckerman  \n",
       "0       0        0      0       0        0          0     0          0  \n",
       "1       0        0      0       0        0          0     0          0  \n",
       "2       0        0      0       0        0          0     0          0  \n",
       "3       0        1      0       0        0          0     0          0  \n",
       "4       0        0      0       0        0          0     0          0  \n",
       "..    ...      ...    ...     ...      ...        ...   ...        ...  \n",
       "59      0        0      0       2        2          0     0          0  \n",
       "60      0        0      0       0        0          0     0          3  \n",
       "61      0        0      0       0        0          0     0          0  \n",
       "62      0        0      0       0        0          0     0          0  \n",
       "63      0        0      0       0        0          0     1          0  \n",
       "\n",
       "[64 rows x 9724 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctm_maker(df.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 04",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q5'></a>\n",
    "\n",
    "### Question 5:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "Now, we will use our dataframe from above to train a `LogisticRegression` classifier on the labeled sentiment.  Here, we can also use the coefficients of the fit model to understand the \"importance\" of features.  Accordingly, we can consider features with high absolute value coefficients as \"important\"; negatively or positively as such.  Your correct solution should generate the table below.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "| tokens\t| coefs| \n",
    "| -------- | -------- |\n",
    "| great\t| 0.8791126599377999| \n",
    "| us\t| 0.6056992793234783| \n",
    "| well\t| 0.5752492839208284| \n",
    "| best\t| 0.5504774974120047| \n",
    "| my\t| 0.5414566720604057| \n",
    "| short\t| 0.5071974167589889| \n",
    "| always\t| 0.5033530077459032| \n",
    "| liked\t| 0.493808270195112| \n",
    "| excellent\t| 0.4874796582725917| \n",
    "| most\t| 0.47949228301494345| \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Write a function, `important_words` that accepts a `X` and `y` which are both `pandas.core.series.Series` objects.\n",
    "    - These objects both contain string dtypes for their values\n",
    "Your function should use the LogisticRegression class to fit the classifier on our `imdb` reviews (text) as input and sentiment as target.\n",
    "    - Note that sentiment values will be strings, `positive` and `negative`\n",
    "    - When instantiating the LogisticRegression class, use the following values:\n",
    "        - `solver=lbfgs`\n",
    "        - `max_iter=2000`\n",
    "        - `random_state=24`\n",
    "Your function should save the resulting coefficients and their tokens as a DataFrame and return the top 10 coeffients (see above table sample)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def important_words(X, y):\n",
    "    return\n",
    "\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question05",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q6'></a>\n",
    "\n",
    "### Question 6:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "Up to this point, we have not done any hyperparameter tuning, nor did we consider different models when building the sentiment classifier.  Instead, we assume that we have arrived at a solid model, and reuse this model on new data to provide sentiment scores.  \n",
    "\n",
    "This is where we call on `textblob`.   Here, we can use a model that itself was trained on the full IMDB dataset to determine sentiment.  We demonstrate the use of the `TextBlob` class, and its sentiment attribute below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "text = '''\n",
    "The titular threat of The Blob has always struck me as the ultimate movie\n",
    "monster: an insatiably hungry, amoeba-like mass able to penetrate\n",
    "virtually any safeguard, capable of--as a doomed doctor chillingly\n",
    "describes it--\"assimilating flesh on contact.\n",
    "Snide comparisons to gelatin be damned, it's a concept with the most\n",
    "devastating of potential consequences, not unlike the grey goo scenario\n",
    "proposed by technological theorists fearful of\n",
    "artificial intelligence run rampant.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "blob = TextBlob(text)\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "According to textblob's documentation:\n",
    "\n",
    "```\n",
    "The sentiment property returns a namedtuple of the form Sentiment(polarity, subjectivity). \n",
    "The polarity score is a float within the range [-1.0, 1.0]. \n",
    "The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "king_rev = pd.read_csv('./data/king_clean.csv', index_col = 0)\n",
    "tommyknockers = king_rev.loc[40, 'review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Using the `tommyknockers` variable above and the TextBlob class, Write a function `pol_sub` that takes the `tommyknockers` variable as an argument. Your function should return a tuple of float values with the first value being the polarity and the second value being the subjectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def pol_sub(text=tommyknockers):\n",
    "    return\n",
    "\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 06",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q7'></a>\n",
    "\n",
    "### Question 7:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "\n",
    "\n",
    "To use `textblob` in assigning sentiment scores to our entire review dataset, we will use the `.apply()` method and apply a custom function that determines both the polarity and subjectivity of the reviews, respectively.  Recall that in Pandas we use the `.apply()` method in place of looping over every row of the dataframe in applying a function to a feature.  A simple example follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "text = '''It was the White Rabbit, trotting slowly back again and looking\n",
    "anxiously about as it went, as if it had lost something; Alice heard it\n",
    "muttering to itself, \"The Duchess! The Duchess! Oh, my dear paws! Oh, my\n",
    "fur and whiskers! She'll get me executed, as sure as ferrets are\n",
    "ferrets! Where _can_ I have dropped them, I wonder?\" Alice guessed in a\n",
    "moment that it was looking for the fan and the pair of white kid-gloves\n",
    "and she very good-naturedly began hunting about for them, but they were\n",
    "nowhere to be seen--everything seemed to have changed since her swim in\n",
    "the pool, and the great hall, with the glass table and the little door,\n",
    "had vanished completely.'''\n",
    "\n",
    "df = pd.DataFrame([text], columns = ['Text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#creating a list of characters using the .split() method\n",
    "df['Text'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#counting the characters\n",
    "df['Text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def num_tokens(string):\n",
    "    from nltk import word_tokenize\n",
    "    return len(word_tokenize(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "king_rev['review'].apply(num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Write a function `num_tokens` that accepts a string as an argument and returns an integer number of tokens. \n",
    "\n",
    "Write another function `series_tokens` that accepts a `pandas.core.series.Series` object as an argument.\n",
    "        - This Series object is assumed to have string values without nulls\n",
    "`series_tokens` should use the `.apply` method in conjunction with the `num_tokens` function to return another series object that has the corresponding integer number of tokens for that row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def num_tokens(string):\n",
    "    return\n",
    "\n",
    "def series_tokens(series):\n",
    "    return\n",
    "\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 07",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q8'></a>\n",
    "\n",
    "### Question 8:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "\n",
    "\n",
    "Write a function `series_pol_sub` that accepts a `pandas.core.series.Series` object as an input.\n",
    "        - Assume this Series object has non-null string values\n",
    "Your function should call the `pol_sub` function you wrote earlier with an `.apply` method.\n",
    "Your function should return a Series object containing a tuple which contains:\n",
    "        - polarity (float)\n",
    "        - subjectivity (float)\n",
    "        \n",
    "The result (snippet) of calling `series_pol_sub` with `king_rev['review']` should be a Series that looks something like this:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "|-|-|\n",
    "|---|---|\n",
    "|0|(0.06928269905881844, 0.48262103982253235)|\n",
    "|1|(0.07120610870610873, 0.5133349366682699)|\n",
    "|2|(0.05425619070049119, 0.46552828432362114)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def series_pol_sub(series):\n",
    "    return\n",
    "\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 08",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q9'></a>\n",
    "\n",
    "### Question 9:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "Similar to the lectures, we can compute a measure of complexity through the lexical diversity of a review.  Here, we define the lexical diversity as the percentage of unique words or characters.  Here, we will rely on the earlier `word_tokenize` operation and assume that we are taking in a list of characters.  We will define lexical diversity as follows:\n",
    "\n",
    "$$\\text{Lexical Diversity} = \\frac{\\text{number unique tokens}}{\\text{number total tokens}}$$\n",
    "\n",
    "For us, this means:\n",
    "\n",
    "- `tokens_u` = number of unique tokens in list resulting from `word_tokenize` applied to text.\n",
    "- `tokens_t` = total number of tokens in list resulting from `word_tokenize` applied to text.\n",
    "\n",
    "Write a function `complexity_score` that takes a multi-token string as an argument (i.e. 'The quick brown fox jumped over the lazy dog').\n",
    "Your function should return a complexity score  as a float.\n",
    "- _Hint_ : The `set` data structure is one way to 'de-dupe' (retrieve unique records) from a `list`.\n",
    "- _Hint_ : Use the `word_tokenize` function from the `nltk` library to parse your input string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def complexity_score(string):\n",
    "    return\n",
    "\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question09",
     "locked": true,
     "points": "1",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q10'></a>\n",
    "\n",
    "### Question 10:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "In our earlier complexity calculation and in our `LogisticRegression` classifier, we did not pay much attention to the elements in our word tokens.  Not only did we include things like punctuation, but we also relied on some potentially redundant vocabulary such as **\"this, the, that, or, and, but, ...\".**  As we saw in the lectures, these are called \"stopwords\".  NLTK provides us a list of stopwords that we can utilize to filter stopwords from a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#create a list of stopwords\n",
    "stops = stopwords.words('english')\n",
    "#print every twentieth stopword\n",
    "print(stops[::20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Write a function `remove_stopwords` that takes a string as an input. Your function should return a `list` of string tokens as an ouput with English stopwords removed.\n",
    "\n",
    "- _Hint_ : You will need to:\n",
    "    - Tokenize your input text with `word_tokenize`\n",
    "    - Remove the token if it is exists within the list of `nltk`'s English stopwords (this is case sensitive!)\n",
    "    - Return the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def remove_stopwords(string):\n",
    "    return\n",
    "\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question10",
     "locked": true,
     "points": "1",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q11'></a>\n",
    "\n",
    "### Question 11:\n",
    "\n",
    "*10 points*\n",
    "\n",
    "After we've removed the stopwords, we can focus on eliminating non-alphanumic characters as a way to eliminate punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "character = ['!', '@', '#', '$', '%', '.', 'Tony']\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Write a function `remove_punctuation_and_stopwords` that accepts a string as an input.\n",
    "Your function should return a `list` of string tokens with English stopwords removed and punctuation removed (as dictated by `str.isalpha`).\n",
    "        \n",
    "_Example:_ `\"There's certainly too much pepper in that soup!\"` --> `{'certainly', 'soup', 'pepper', 'much'}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def remove_punctuation_and_stopwords(string):\n",
    "    return\n",
    "\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 11",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "<a id='q12'></a>\n",
    "\n",
    "### Question 12:\n",
    "\n",
    "Now, using our ealier defined functions, we should be able to take in a feature of text, remove the stopwords, and compute the complexity score for a review.  We apply this to our Stephen King reviews, and might interpret the solution as speaking to the diversity of language the book induced in a critic.\n",
    "\n",
    "Write a function `clean_complexity` that accepts a `pandas.core.series.Series` object as an argument. Assume the values of this series are non-null strings.\n",
    "   \n",
    "Your function should remove all stop words and punctuation from the values in this series and return a `Series` object whose values are the corresponding lexical diversity score (float) for each input row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def clean_complexity(series):\n",
    "    return\n",
    "\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question12",
     "locked": true,
     "points": "1",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### What's the right Stephen King book?\n",
    "\n",
    "Now, we might use our function to approximate the best book to read based on the diversity of language excited from the critic.  There are many additional approaches for conducting such an analysis, our aim was to give you insight into how some of these tools work.  You can adjust things like the words included and excluded in training a sentiment model or computing complexity, consider ngrams, only certain parts of speech, etc.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "king_rev = pd.read_csv('./data/king_clean.csv', index_col=0, nrows=100)\n",
    "king_rev['complexity'] = clean_complexity(king_rev['review'])\n",
    "king_rev.sort_values('complexity', ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
